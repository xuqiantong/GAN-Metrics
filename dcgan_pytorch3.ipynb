{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan_pytorch3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylematoba/GAN-Metrics/blob/master/dcgan_pytorch3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTrMcMltRSQH",
        "colab_type": "code",
        "outputId": "ef2626e5-2922-4dc4-b553-2112466b6ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install --upgrade setuptools"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvH7qnpmTI5A",
        "colab_type": "code",
        "outputId": "8f168ba4-86ca-4efe-8d08-e1fdc7aec775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !rm -rf examples\n",
        "!git clone https://github.com/kylematoba/examples.git\n",
        "# !git -C examples log -n 2"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'examples' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfsI5MSHQSJ3",
        "colab_type": "code",
        "outputId": "09f7f7a0-9ba9-4f64-b35e-87785d08f3b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !rm -rf gan_metrics\n",
        "!git clone https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/GAN-Metrics.git gan_metrics"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gan_metrics' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZauQnzqJ4vJ",
        "colab_type": "code",
        "outputId": "ac152483-cc4b-44d7-a776-11cc06c53cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "!pip install --upgrade git+https://github.com/Lyken17/pytorch-OpCounter.git\n",
        "import thop"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Lyken17/pytorch-OpCounter.git\n",
            "  Cloning https://github.com/Lyken17/pytorch-OpCounter.git to /tmp/pip-req-build-0lr46jaj\n",
            "  Running command git clone -q https://github.com/Lyken17/pytorch-OpCounter.git /tmp/pip-req-build-0lr46jaj\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from thop==0.0.22) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch->thop==0.0.22) (1.16.3)\n",
            "Building wheels for collected packages: thop\n",
            "  Building wheel for thop (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kq9e96fs/wheels/79/0e/29/2d013ff0d3e36ae48894c11a6a9eecad6bc4789849f5ed802a\n",
            "Successfully built thop\n",
            "Installing collected packages: thop\n",
            "  Found existing installation: thop 0.0.22\n",
            "    Uninstalling thop-0.0.22:\n",
            "      Successfully uninstalled thop-0.0.22\n",
            "Successfully installed thop-0.0.22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "thop"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT5mwRBMOrt-",
        "colab_type": "code",
        "outputId": "295bce0c-0df8-4d68-a490-901ab11e215f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!pip3 install pot"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pot in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pot) (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pot) (3.0.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from pot) (0.29.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pot) (1.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->pot) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->pot) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdnOpHY1kl7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import sys\n",
        "import logging\n",
        "import warnings\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import gan_metrics.metric as metric\n",
        "\n",
        "FORMAT = \"%(asctime)s %(process)s %(thread)s: %(message)s\"\n",
        "logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "to_addrs = ['kylematoba@gmail.com']\n",
        "dict_environ = dict(os.environ)\n",
        "# logger.info(pprint.pformat(dict_environ, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p71Nrh9AxBkB",
        "colab_type": "code",
        "outputId": "7dbb383e-345a-4fe8-8e49-8f39952b6c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "!rm -rf matobapythong\n",
        "!pip3 install --force-reinstall git+https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/matobapython.git"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://kylematoba:****@github.com/kylematoba/matobapython.git\n",
            "  Cloning https://kylematoba:****@github.com/kylematoba/matobapython.git to /tmp/pip-req-build-dtw1j7g4\n",
            "  Running command git clone -q 'https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/matobapython.git' /tmp/pip-req-build-dtw1j7g4\n",
            "Building wheels for collected packages: matobapython\n",
            "  Building wheel for matobapython (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1h1vhqnd/wheels/e1/2d/7a/3c81733c70f1f3d702f15d4d9f352f995deacb0ee96b476c47\n",
            "Successfully built matobapython\n",
            "Installing collected packages: matobapython\n",
            "  Found existing installation: matobapython 0.0.1\n",
            "    Uninstalling matobapython-0.0.1:\n",
            "      Successfully uninstalled matobapython-0.0.1\n",
            "Successfully installed matobapython-0.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pythonutils"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w_lu9eoxF9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pythonutils.gdrive as gdrive\n",
        "import pythonutils.plotting as plotting\n",
        "import pythonutils.send_email as send_email"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhveQGed9tZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_names = np.array(['pixl_wasserstein', 'pixl_mmd', 'pixl_acc', 'pixl_acc_t',\n",
        "                         'pixl_acc_f', 'pixl_precision', 'pixl_recall', 'conv_wasserstein',\n",
        "                         'conv_mmd', 'conv_acc', 'conv_acc_t', 'conv_acc_f',\n",
        "                         'conv_precision', 'conv_recall', 'logit_wasserstein', 'logit_mmd',\n",
        "                         'logit_acc', 'logit_acc_t', 'logit_acc_f', 'logit_precision',\n",
        "                         'logit_recall', 'smax_wasserstein', 'smax_mmd', 'smax_acc',\n",
        "                         'smax_acc_t', 'smax_acc_f', 'smax_precision', 'smax_recall',\n",
        "                         'inception_score', 'mode_score', 'fid'], dtype=object)\n",
        "\n",
        "load_metrics = ['conv_wasserstein', 'conv_mmd', 'conv_acc', 'conv_acc_t', \n",
        "                'conv_acc_f', 'conv_precision', 'conv_recall', \n",
        "                'inception_score', 'mode_score', 'fid']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOVreYHUJq6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _delete_all_remote_files(del_filename: str, \n",
        "                             parent_fid: str) -> None:\n",
        "    del_files = gdrive.find_items(name=del_filename, \n",
        "                                  parent_fid=parent_fid, \n",
        "                                  skip_trashed=True)\n",
        "    for x in del_files:\n",
        "        logger.info(\"Deleting {}\".format(x))\n",
        "        delete_fid = x[1]\n",
        "        gdrive.delete_file(delete_fid)\n",
        "        \n",
        "        \n",
        "def _create_folder_in_parent(folder_name: str, \n",
        "                             parent_fid: str, \n",
        "                             exist_ok: bool) -> str:\n",
        "    found_folders = gdrive.find_items(folder_name, \n",
        "                                      parent_fid, \n",
        "                                      skip_trashed=True)\n",
        "    num_found = len(found_folders)\n",
        "    assert num_found <= 1, \"Multiple matches, refine query\"\n",
        "\n",
        "    if 1 == num_found:\n",
        "        found_folder = found_folders[0] \n",
        "        folder_fid = found_folder[1]\n",
        "        logger.info(\"Found it, {}\".format(folder_fid))\n",
        "        assert exist_ok, \"Not expecting to find it\"\n",
        "    else:\n",
        "        created_folder = gdrive.create_folder(folder_name=folder_name, \n",
        "                                              parent_fid=parent_fid)\n",
        "        folder_fid = created_folder[1]\n",
        "        logger.info(\"Not found, created with fid = {}\".format(folder_fid))\n",
        "    return folder_fid\n",
        "\n",
        "  \n",
        "def _get_epoch_from_checkpoint(x: str) -> int:\n",
        "    return int(x.rstrip('.pth').split('epoch')[-1])\n",
        "  \n",
        "  \n",
        "def _download_file_locally(filename: str, \n",
        "                           parent_fid: str) -> None:\n",
        "    remote_files = gdrive.find_items(filename, \n",
        "                                     parent_fid, \n",
        "                                     skip_trashed=True)\n",
        "    assert 1 == len(remote_files), str(remote_files)\n",
        "    remote_file = remote_files[0]\n",
        "    remote_fid = remote_file[1]\n",
        "    gdrive.download_file_to_folder(remote_fid, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3FS5H51upmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _get_dataset(dataset_name: str, \n",
        "                 image_size: int) -> torch.utils.data.Dataset:\n",
        "    if dataset_name in ['imagenet', 'folder', 'lfw']:\n",
        "        dataset = dset.ImageFolder(root=dataroot,\n",
        "                                   transform=transforms.Compose([\n",
        "                                       transforms.Resize(image_size),\n",
        "                                       transforms.CenterCrop(image_size),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                                   ]))\n",
        "    elif dataset_name == 'lsun':\n",
        "        dataset = dset.LSUN(root=dataroot, classes=['bedroom_train'],\n",
        "                            transform=transforms.Compose([\n",
        "                                transforms.Resize(image_size),\n",
        "                                transforms.CenterCrop(image_size),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                            ]))\n",
        "    elif dataset_name == 'cifar10':\n",
        "        dataset = dset.CIFAR10(root=dataroot, download=True,\n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.Resize(image_size),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                               ]))\n",
        "    elif dataset_name == 'mnist':\n",
        "            dataset = dset.MNIST(root=dataroot, download=True,\n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.Resize(image_size),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5,), (0.5,)),\n",
        "                               ]))\n",
        "            nc=1\n",
        "    elif dataset_name == 'fake':\n",
        "        dataset = dset.FakeData(image_size=(3, image_size, image_size),\n",
        "                                transform=transforms.ToTensor())\n",
        "\n",
        "    assert dataset\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu: int):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu: int):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "\n",
        "\n",
        "    \n",
        "def _get_net_filenames(base_folder_name: str, \n",
        "                       dataset_name: str,\n",
        "                       zstr: str,\n",
        "                       seedstr: str,\n",
        "                       out_folder_local: str) -> Tuple[str, str, int]:\n",
        "    logger.info(\"Setting up base folder '{}'\".format(base_folder_name))\n",
        "    base_folder_fid = _create_folder_in_parent(folder_name=base_folder_name, \n",
        "                                               parent_fid=None, \n",
        "                                               exist_ok=True)\n",
        "\n",
        "    logger.info(\"Setting up dataset folder '{}' in '{}'\".format(dataset_name, base_folder_name))\n",
        "    dataset_folder_fid = _create_folder_in_parent(folder_name=dataset_name, \n",
        "                                                  parent_fid=base_folder_fid, \n",
        "                                                  exist_ok=True)\n",
        "\n",
        "    logger.info(\"Setting up dataset/z/ folder '{}' in '{}'\".format(zstr, dataset_name))\n",
        "    datasetz_folder_fid = _create_folder_in_parent(folder_name=zstr, \n",
        "                                                   parent_fid=dataset_folder_fid, \n",
        "                                                   exist_ok=True)\n",
        "\n",
        "    logger.info(\"Setting up dataset/z/seed folder '{}' in '{}'\".format(seedstr, zstr))\n",
        "    datasetzseed_folder_fid = _create_folder_in_parent(folder_name=seedstr, \n",
        "                                                       parent_fid=datasetz_folder_fid, \n",
        "                                                       exist_ok=True)\n",
        "\n",
        "    parent_fid = datasetzseed_folder_fid\n",
        "\n",
        "    save_folder_r = os.path.join(out_folder_local, 'real/')\n",
        "    save_folder_f = os.path.join(out_folder_local, 'fake/')\n",
        "\n",
        "    os.makedirs(save_folder_r, exist_ok=True)\n",
        "    os.makedirs(save_folder_f, exist_ok=True)\n",
        "\n",
        "    net_g_items = gdrive.find_items(name=net_g_pattern, parent_fid=parent_fid, skip_trashed=True)\n",
        "    net_d_items = gdrive.find_items(name=net_d_pattern, parent_fid=parent_fid, skip_trashed=True)\n",
        "    sorted_net_g_filenames = sorted([x[0] for x in net_g_items])\n",
        "    sorted_net_d_filenames = sorted([x[0] for x in net_d_items])\n",
        "\n",
        "    if attempt_reload and len(sorted_net_g_filenames) > 2 and len(sorted_net_d_filenames) > 2:\n",
        "        latest_net_g_filename = max(sorted_net_g_filenames)\n",
        "        latest_net_d_filename = max(sorted_net_d_filenames)\n",
        "\n",
        "        latest_net_g_epoch = _get_epoch_from_checkpoint(latest_net_g_filename)\n",
        "        latest_net_d_epoch = _get_epoch_from_checkpoint(latest_net_d_filename)\n",
        "\n",
        "        latest_epoch = min(latest_net_g_epoch, latest_net_d_epoch)\n",
        "\n",
        "        net_g_filename = checkpoint_pattern.format(net_g_pattern, latest_epoch)\n",
        "        net_d_filename = checkpoint_pattern.format(net_d_pattern, latest_epoch)\n",
        "\n",
        "        last_net_g_fullfilename = sorted_net_g_filenames[sorted_net_g_filenames.index(net_g_filename) - 1]\n",
        "        last_net_d_fullfilename = sorted_net_d_filenames[sorted_net_d_filenames.index(net_d_filename) - 1]\n",
        "\n",
        "        g_epoch = _get_epoch_from_checkpoint(last_net_g_fullfilename)\n",
        "        d_epoch = _get_epoch_from_checkpoint(last_net_d_fullfilename)\n",
        "\n",
        "        load_epoch = min(d_epoch, g_epoch)\n",
        "        if max_load is not None:\n",
        "           load_epoch = min(load_epoch, max_load)\n",
        "\n",
        "        net_g_fullfilename = checkpoint_pattern.format(net_g_pattern, load_epoch)\n",
        "        net_d_fullfilename = checkpoint_pattern.format(net_d_pattern, load_epoch)\n",
        "\n",
        "        _download_file_locally(net_d_fullfilename, parent_fid)\n",
        "        _download_file_locally(net_g_fullfilename, parent_fid)\n",
        "        min_iter = load_epoch\n",
        "        logger.info(\"Loading from epoch {:04d}\".format(load_epoch))\n",
        "\n",
        "    else:\n",
        "        net_g_fullfilename = ''\n",
        "        net_d_fullfilename = ''\n",
        "\n",
        "        min_iter = 0\n",
        "\n",
        "    return net_g_fullfilename, net_d_fullfilename, min_iter, parent_fid\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gixNpvmqKThK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# environment configuration, not model related\n",
        "dataroot = 'examples'\n",
        "\n",
        "out_folder_local = '/content'\n",
        "username = 'robotmatoba'\n",
        "metrics_pattern = \"metrics_{:04d}.npy\"\n",
        "\n",
        "attempt_reload = True\n",
        "# attempt_reload = False\n",
        "# max_load = 0\n",
        "max_load = None\n",
        "\n",
        "base_folder_name = 'PytorchCheckpoints'\n",
        "\n",
        "print_every_iteration = 250\n",
        "save_every_iteration = 250\n",
        "# checkpoint_every_epoch = 2\n",
        "checkpoint_every_epoch = 4\n",
        "\n",
        "net_g_pattern = 'netG_epoch'\n",
        "net_d_pattern = 'netD_epoch'\n",
        "\n",
        "checkpoint_pattern = '{}{:04d}.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irLFxc4dkpKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# seed_char = 'a'  # 01\n",
        "# seed_char = 'b'  # 02\n",
        "# seed_char = 'c'  # 03\n",
        "# seed_char = 'd'  # 04\n",
        "# seed_char = 'e'  # 05\n",
        "# seed_char = 'f'  # 06\n",
        "# seed_char = 'g'  # 07\n",
        "# seed_char = 'h'  # 08\n",
        "# seed_char = 'i'  # 09\n",
        "# seed_char = 'j'  # 10\n",
        "# seed_char = 'k'  # 11\n",
        "# seed_char = 'l'  # 12\n",
        "# seed_char = 'm'  # 13\n",
        "# seed_char = 'n'  # 14\n",
        "# seed_char = 'o'  # 15\n",
        "# seed_char = 'p'  # 16\n",
        "# seed_char = 'q'  # 17\n",
        "seed_char = 'r'  # 18\n",
        "# seed_char = 's'  # 19\n",
        "# seed_char = 't'  # 20\n",
        "\n",
        "prng_seed = ord(seed_char)\n",
        "\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "\n",
        "lr = 0.0002\n",
        "beta1 = .5\n",
        "\n",
        "# max_iter = 3\n",
        "# max_iter = 10\n",
        "max_iter = 25\n",
        "\n",
        "num_workers = 2\n",
        "ngpu = 1\n",
        "\n",
        "# nz = 1\n",
        "# nz = 2\n",
        "# nz = 5\n",
        "# nz = 10\n",
        "nz = 25\n",
        "# nz = 100\n",
        "# nz = 200\n",
        "\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "if prng_seed is None:\n",
        "    prng_seed = random.randint(1, 10000)\n",
        "\n",
        "zstr = 'z{:05d}'.format(nz)\n",
        "seedstr = 'seed_{}'.format(seed_char)\n",
        "\n",
        "\n",
        "# dataset_name = 'lsun'\n",
        "# dataset_name = 'mnist'\n",
        "dataset_name = 'cifar10'\n",
        "\n",
        "identifier = dataset_name + zstr + seedstr\n",
        "\n",
        "sample_size = 2000\n",
        "\n",
        "if dataset_name == 'mnist':\n",
        "    nc = 1\n",
        "else:\n",
        "    nc = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxciv29hTna5",
        "colab_type": "code",
        "outputId": "d6a210ff-10e3-47b3-c161-8a22c9994c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "logger.info(\"Random Seed: {}\".format(prng_seed))\n",
        "random.seed(prng_seed)\n",
        "torch.manual_seed(prng_seed)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "dataset = _get_dataset(dataset_name, image_size)\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "netG = Generator(ngpu).to(device).apply(weights_init)\n",
        "netD = Discriminator(ngpu).to(device).apply(weights_init)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-08 10:13:37,599 1762 140435581867904: Random Seed: 114\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaX2RRDasY0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gdrive.authenticate_automatically(username)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrcWGX_othO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_fitting(netG: Generator, \n",
        "               netD: Discriminator, \n",
        "               lr: float,\n",
        "               beta1: float,\n",
        "               min_iter: int,\n",
        "               max_iter: int,\n",
        "               nz: int,\n",
        "               batch_size: int,\n",
        "               device: torch.device,\n",
        "               dataset: torch.utils.data.Dataset, \n",
        "               checkpoint_dir: str,\n",
        "               out_folder_local: str, \n",
        "               parent_fid: str) -> Tuple[Any]: \n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    os.makedirs(out_folder_local, exist_ok=True)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    real_label = 1\n",
        "    fake_label = 0\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=num_workers)\n",
        "\n",
        "    optimizer_d = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "    optimizer_g = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "    dataloader_size = len(dataloader)\n",
        "    \n",
        "    logger.info(\"Running iterations {} to {}\".format(min_iter, max_iter))\n",
        "    \n",
        "    for epoch in range(min_iter, max_iter):\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "            # train with real\n",
        "            netD.zero_grad()\n",
        "            real_cpu = data[0].to(device)\n",
        "            batch_size = real_cpu.size(0)\n",
        "            label = torch.full((batch_size, ), real_label, device=device)\n",
        "\n",
        "            output = netD(real_cpu)\n",
        "            errD_real = criterion(output, label)\n",
        "            errD_real.backward()\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "            # train with fake\n",
        "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "            fake = netG(noise)\n",
        "            label.fill_(fake_label)\n",
        "            output = netD(fake.detach())\n",
        "            errD_fake = criterion(output, label)\n",
        "            errD_fake.backward()\n",
        "            D_G_z1 = output.mean().item()\n",
        "            errD = errD_real + errD_fake\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # (2) Update G network: maximize log(D(G(z)))\n",
        "            netG.zero_grad()\n",
        "            label.fill_(real_label)  # fake labels are real for generator cost\n",
        "            output = netD(fake)\n",
        "            errG = criterion(output, label)\n",
        "            errG.backward()\n",
        "            D_G_z2 = output.mean().item()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            if i % print_every_iteration == 0:\n",
        "                loss_d = errD.item()\n",
        "                loss_g = errG.item()\n",
        "\n",
        "                logger.info('[%d/%d] [%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "                      % (epoch, max_iter, i, dataloader_size, loss_d, loss_g, D_x, D_G_z1, D_G_z2))\n",
        "            if i % save_every_iteration == 0:\n",
        "                real_filename = os.path.join(out_folder_local, 'real_samples.png')\n",
        "                fake_filename = os.path.join(out_folder_local, 'fake_samples_epoch_{:03d}.png'.format(epoch))\n",
        "\n",
        "                fake = netG(fixed_noise)\n",
        "                \n",
        "                vutils.save_image(real_cpu, real_filename, normalize=True)\n",
        "                vutils.save_image(fake.detach(), fake_filename, normalize=True)\n",
        "                \n",
        "        net_g_filename = checkpoint_pattern.format(net_g_pattern, epoch)\n",
        "        net_d_filename = checkpoint_pattern.format(net_d_pattern, epoch)\n",
        "\n",
        "        net_g_full_filename = os.path.join(checkpoint_dir, net_g_filename)\n",
        "        net_d_full_filename = os.path.join(checkpoint_dir, net_d_filename)\n",
        "\n",
        "        torch.save(netG.state_dict(), net_g_full_filename)\n",
        "        torch.save(netD.state_dict(), net_d_full_filename)\n",
        "\n",
        "        if 0 == epoch % checkpoint_every_epoch:\n",
        "            logging.getLogger().setLevel(logging.ERROR)\n",
        "            logger.info('Checkpointing epoch {}'.format(epoch))\n",
        "\n",
        "            _delete_all_remote_files(net_g_filename, parent_fid=parent_fid)\n",
        "            _delete_all_remote_files(net_d_filename, parent_fid=parent_fid)\n",
        "            \n",
        "            gdrive.upload_file_to_folder(local_file=net_g_filename, parent_fid=parent_fid)\n",
        "            gdrive.upload_file_to_folder(local_file=net_d_filename, parent_fid=parent_fid)\n",
        "            logging.getLogger().setLevel(logging.INFO)\n",
        "    return netG, netD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iduaJTMGzvTy",
        "colab_type": "code",
        "outputId": "b51de8bc-e6b2-4633-871d-37bfcdd64397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1253
        }
      },
      "source": [
        "net_g_fullfilename, net_d_fullfilename, min_iter, parent_fid = _get_net_filenames(base_folder_name, dataset_name, zstr, seedstr, out_folder_local)\n",
        "\n",
        "if net_g_fullfilename != '':\n",
        "    netG.load_state_dict(torch.load(net_g_fullfilename))\n",
        "\n",
        "if net_d_fullfilename != '':\n",
        "    netD.load_state_dict(torch.load(net_d_fullfilename))\n",
        "\n",
        "checkpoint_dir = out_folder_local\n",
        "netG, netD = do_fitting(netG, netD, lr, beta1, min_iter, max_iter, nz, batch_size, device, dataset, checkpoint_dir, out_folder_local, parent_fid)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-08 10:13:38,960 1762 140435581867904: Setting up base folder 'PytorchCheckpoints'\n",
            "2019-05-08 10:13:38,962 1762 140435581867904: Submitting query 'name contains \"PytorchCheckpoints\" and trashed = false'\n",
            "2019-05-08 10:13:38,969 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22PytorchCheckpoints%22+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:13:39,399 1762 140435581867904: Found it, 1t4s77dg0G5hm0TlHvnrQkBYpJ64wuHml\n",
            "2019-05-08 10:13:39,400 1762 140435581867904: Setting up dataset folder 'cifar10' in 'PytorchCheckpoints'\n",
            "2019-05-08 10:13:39,401 1762 140435581867904: Submitting query 'name contains \"cifar10\" and \"1t4s77dg0G5hm0TlHvnrQkBYpJ64wuHml\" in parents and trashed = false'\n",
            "2019-05-08 10:13:39,411 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22cifar10%22+and+%221t4s77dg0G5hm0TlHvnrQkBYpJ64wuHml%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:13:39,741 1762 140435581867904: Found it, 1_3XHVYiPP8uoSvr-J5SYE97or8rgCecF\n",
            "2019-05-08 10:13:39,742 1762 140435581867904: Setting up dataset/z/ folder 'z00025' in 'cifar10'\n",
            "2019-05-08 10:13:39,743 1762 140435581867904: Submitting query 'name contains \"z00025\" and \"1_3XHVYiPP8uoSvr-J5SYE97or8rgCecF\" in parents and trashed = false'\n",
            "2019-05-08 10:13:39,751 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22z00025%22+and+%221_3XHVYiPP8uoSvr-J5SYE97or8rgCecF%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:13:40,091 1762 140435581867904: Found it, 1Ts4MMw6LiCaFfy4JfzW4WUk9wSeht14z\n",
            "2019-05-08 10:13:40,092 1762 140435581867904: Setting up dataset/z/seed folder 'seed_r' in 'z00025'\n",
            "2019-05-08 10:13:40,094 1762 140435581867904: Submitting query 'name contains \"seed_r\" and \"1Ts4MMw6LiCaFfy4JfzW4WUk9wSeht14z\" in parents and trashed = false'\n",
            "2019-05-08 10:13:40,102 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22seed_r%22+and+%221Ts4MMw6LiCaFfy4JfzW4WUk9wSeht14z%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:13:40,419 1762 140435581867904: Found it, 17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\n",
            "2019-05-08 10:13:40,420 1762 140435581867904: Submitting query 'name contains \"netG_epoch\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:13:40,426 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_epoch%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:13:40,764 1762 140435581867904: Submitting query 'name contains \"netD_epoch\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:13:40,768 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_epoch%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:13:41,066 1762 140435581867904: Submitting query 'name contains \"netD_epoch0020.pth\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:13:41,073 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_epoch0020.pth%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:13:41,391 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files/141dLOi6tBiSPTiZ9w1Fo8yILb1GKXGOv?alt=media\n",
            "2019-05-08 10:13:45,332 1762 140435581867904: Submitting query 'name contains \"netG_epoch0020.pth\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:13:45,337 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_epoch0020.pth%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:13:45,666 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files/1SsMGg4PhlfRdEtRXKQXYy9RmAMzkPIKT?alt=media\n",
            "2019-05-08 10:13:48,048 1762 140435581867904: Loading from epoch 0020\n",
            "2019-05-08 10:13:48,076 1762 140435581867904: Running iterations 20 to 25\n",
            "2019-05-08 10:13:48,356 1762 140435581867904: [20/25] [0/782] Loss_D: 0.1100 Loss_G: 9.1214 D(x): 0.9931 D(G(z)): 0.0914 / 0.0002\n",
            "2019-05-08 10:14:01,917 1762 140435581867904: [20/25] [200/782] Loss_D: 0.6114 Loss_G: 3.4522 D(x): 0.9130 D(G(z)): 0.3597 / 0.0469\n",
            "2019-05-08 10:14:15,638 1762 140435581867904: [20/25] [400/782] Loss_D: 0.9524 Loss_G: 1.8264 D(x): 0.5667 D(G(z)): 0.1865 / 0.2137\n",
            "2019-05-08 10:14:29,494 1762 140435581867904: [20/25] [600/782] Loss_D: 0.0725 Loss_G: 5.1202 D(x): 0.9786 D(G(z)): 0.0481 / 0.0089\n",
            "2019-05-08 10:14:42,383 1762 140435581867904: Checkpointing epoch 20\n",
            "2019-05-08 10:14:42,386 1762 140435581867904: Submitting query 'name contains \"netG_epoch0020.pth\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:14:42,394 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_epoch0020.pth%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:14:42,849 1762 140435581867904: Deleting ('netG_epoch0020.pth', '1SsMGg4PhlfRdEtRXKQXYy9RmAMzkPIKT')\n",
            "2019-05-08 10:14:42,856 1762 140435581867904: URL being requested: DELETE https://www.googleapis.com/drive/v3/files/1SsMGg4PhlfRdEtRXKQXYy9RmAMzkPIKT?\n",
            "2019-05-08 10:14:44,086 1762 140435581867904: Submitting query 'name contains \"netD_epoch0020.pth\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:14:44,090 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_epoch0020.pth%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:14:44,647 1762 140435581867904: Deleting ('netD_epoch0020.pth', '141dLOi6tBiSPTiZ9w1Fo8yILb1GKXGOv')\n",
            "2019-05-08 10:14:44,650 1762 140435581867904: URL being requested: DELETE https://www.googleapis.com/drive/v3/files/141dLOi6tBiSPTiZ9w1Fo8yILb1GKXGOv?\n",
            "2019-05-08 10:14:45,479 1762 140435581867904: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n",
            "2019-05-08 10:14:47,344 1762 140435581867904: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n",
            "2019-05-08 10:14:49,535 1762 140435581867904: [21/25] [0/782] Loss_D: 0.8706 Loss_G: 2.3920 D(x): 0.8794 D(G(z)): 0.4595 / 0.1257\n",
            "2019-05-08 10:15:03,763 1762 140435581867904: [21/25] [200/782] Loss_D: 0.1837 Loss_G: 4.0434 D(x): 0.9669 D(G(z)): 0.1281 / 0.0256\n",
            "2019-05-08 10:15:18,029 1762 140435581867904: [21/25] [400/782] Loss_D: 0.9267 Loss_G: 1.4804 D(x): 0.5382 D(G(z)): 0.1201 / 0.2966\n",
            "2019-05-08 10:15:32,374 1762 140435581867904: [21/25] [600/782] Loss_D: 0.9568 Loss_G: 1.6353 D(x): 0.5218 D(G(z)): 0.1276 / 0.2504\n",
            "2019-05-08 10:15:45,763 1762 140435581867904: [22/25] [0/782] Loss_D: 0.0540 Loss_G: 4.1252 D(x): 0.9967 D(G(z)): 0.0488 / 0.0196\n",
            "2019-05-08 10:16:00,285 1762 140435581867904: [22/25] [200/782] Loss_D: 0.0294 Loss_G: 5.0915 D(x): 0.9912 D(G(z)): 0.0199 / 0.0107\n",
            "2019-05-08 10:16:14,850 1762 140435581867904: [22/25] [400/782] Loss_D: 0.1198 Loss_G: 5.3677 D(x): 0.9727 D(G(z)): 0.0839 / 0.0073\n",
            "2019-05-08 10:16:29,597 1762 140435581867904: [22/25] [600/782] Loss_D: 0.8571 Loss_G: 2.9916 D(x): 0.7017 D(G(z)): 0.2770 / 0.0859\n",
            "2019-05-08 10:16:43,258 1762 140435581867904: [23/25] [0/782] Loss_D: 0.7669 Loss_G: 2.1168 D(x): 0.6723 D(G(z)): 0.2394 / 0.1712\n",
            "2019-05-08 10:16:58,120 1762 140435581867904: [23/25] [200/782] Loss_D: 0.6980 Loss_G: 3.1418 D(x): 0.8109 D(G(z)): 0.3191 / 0.0668\n",
            "2019-05-08 10:17:13,009 1762 140435581867904: [23/25] [400/782] Loss_D: 0.5161 Loss_G: 3.9749 D(x): 0.9277 D(G(z)): 0.3174 / 0.0263\n",
            "2019-05-08 10:17:27,983 1762 140435581867904: [23/25] [600/782] Loss_D: 0.0313 Loss_G: 5.1023 D(x): 0.9899 D(G(z)): 0.0207 / 0.0092\n",
            "2019-05-08 10:17:41,936 1762 140435581867904: [24/25] [0/782] Loss_D: 0.5304 Loss_G: 3.0480 D(x): 0.8744 D(G(z)): 0.2817 / 0.0655\n",
            "2019-05-08 10:17:57,030 1762 140435581867904: [24/25] [200/782] Loss_D: 1.2959 Loss_G: 4.3208 D(x): 0.9405 D(G(z)): 0.6113 / 0.0279\n",
            "2019-05-08 10:18:12,055 1762 140435581867904: [24/25] [400/782] Loss_D: 0.0513 Loss_G: 3.7476 D(x): 0.9979 D(G(z)): 0.0470 / 0.0362\n",
            "2019-05-08 10:18:27,060 1762 140435581867904: [24/25] [600/782] Loss_D: 0.0318 Loss_G: 6.1858 D(x): 0.9722 D(G(z)): 0.0032 / 0.0030\n",
            "2019-05-08 10:18:40,767 1762 140435581867904: Checkpointing epoch 24\n",
            "2019-05-08 10:18:40,770 1762 140435581867904: Submitting query 'name contains \"netG_epoch0024.pth\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:18:40,778 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_epoch0024.pth%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:18:41,151 1762 140435581867904: Deleting ('netG_epoch0024.pth', '1PPJ3c4_FXLjTFL0Q7RdhCeKPO-Pz73ob')\n",
            "2019-05-08 10:18:41,156 1762 140435581867904: URL being requested: DELETE https://www.googleapis.com/drive/v3/files/1PPJ3c4_FXLjTFL0Q7RdhCeKPO-Pz73ob?\n",
            "2019-05-08 10:18:41,751 1762 140435581867904: Submitting query 'name contains \"netD_epoch0024.pth\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:18:41,755 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_epoch0024.pth%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:18:42,099 1762 140435581867904: Deleting ('netD_epoch0024.pth', '1LWOsM7UdkdXoTojVI3YpJtv9KYDnFEi-')\n",
            "2019-05-08 10:18:42,103 1762 140435581867904: URL being requested: DELETE https://www.googleapis.com/drive/v3/files/1LWOsM7UdkdXoTojVI3YpJtv9KYDnFEi-?\n",
            "2019-05-08 10:18:42,678 1762 140435581867904: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n",
            "2019-05-08 10:18:44,853 1762 140435581867904: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW9WIm2JvMy1",
        "colab_type": "code",
        "outputId": "6cbb58b2-3a0a-41f3-c56a-9abb3c571c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "logging.getLogger().setLevel(logging.INFO)\n",
        "netG = Generator(ngpu).to(device).apply(weights_init)\n",
        "        \n",
        "\n",
        "save_folder_r = os.path.join(out_folder_local, 'real/')\n",
        "save_folder_f = os.path.join(out_folder_local, 'fake/')\n",
        "\n",
        "# [emd-mmd-knn(knn,real,fake,precision,recall)]*4 - IS - mode_score - FID\n",
        "score_tr = np.zeros((max_iter, 4*7+3))\n",
        "\n",
        "for epoch in range(0, max_iter):\n",
        "    metrics_filename = metrics_pattern.format(epoch)\n",
        "    logger.info(metrics_filename)\n",
        "    # print(metrics_filename)\n",
        "    found_items = gdrive.find_items(name=metrics_filename, \n",
        "                                    parent_fid=parent_fid,\n",
        "                                    skip_trashed=True)\n",
        "    if len(found_items) > 0:\n",
        "        # print(len(found_items))\n",
        "        assert len(found_items) <= 1\n",
        "        found_item = found_items[0]\n",
        "        assert found_item[0] == metrics_filename\n",
        "        _download_file_locally(metrics_filename, parent_fid)\n",
        "        s = np.load(metrics_filename)\n",
        "    else:\n",
        "        net_g_filename = checkpoint_pattern.format(net_g_pattern, epoch)\n",
        "        logger.info(\"Computing metrics on {}\".format(net_g_filename))\n",
        "        logger.info(\"Downloading {}\".format(net_g_filename))\n",
        "        try:\n",
        "            # logging.getLogger().setLevel(logging.ERROR)\n",
        "            _download_file_locally(net_g_filename, parent_fid)\n",
        "            netG.load_state_dict(torch.load(net_g_filename))\n",
        "    \n",
        "            s = metric.compute_score_raw(dataset_name, \n",
        "                                         image_size, \n",
        "                                         dataroot, \n",
        "                                         sample_size, \n",
        "                                         batch_size, \n",
        "                                         saveFolder_r=save_folder_r, \n",
        "                                         saveFolder_f=save_folder_f, \n",
        "                                         netG=netG, \n",
        "                                         nz=nz, \n",
        "                                         conv_model='inception_v3', \n",
        "                                         workers=num_workers)\n",
        "            np.save(metrics_filename, s)    \n",
        "            _delete_all_remote_files(metrics_filename, parent_fid=parent_fid)\n",
        "            gdrive.upload_file_to_folder(metrics_filename, parent_fid=parent_fid)\n",
        "            # logging.getLogger().setLevel(logging.INFO)\n",
        "        except:\n",
        "            logger.info(\"Computation of scores for {} failed\".format(net_g_filename))\n",
        "            s = np.nan\n",
        "    score_tr[epoch, :] = s\n",
        "logger.info(\"Done computing scores\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-08 10:38:53,548 1762 140435581867904: metrics_0000.npy\n",
            "2019-05-08 10:38:53,549 1762 140435581867904: Submitting query 'name contains \"metrics_0000.npy\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:38:53,555 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22metrics_0000.npy%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:38:53,890 1762 140435581867904: Computing metrics on netG_epoch0000.pth\n",
            "2019-05-08 10:38:53,891 1762 140435581867904: Downloading netG_epoch0000.pth\n",
            "2019-05-08 10:38:53,892 1762 140435581867904: Submitting query 'name contains \"netG_epoch0000.pth\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:38:53,899 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_epoch0000.pth%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:38:54,195 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files/1fBE5CUCsWfUS0sQw0boBgjAWVbPBjZR6?alt=media\n",
            "sampling real images ...\n",
            "Files already downloaded and verified\n",
            "sampling fake images ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/32 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting features...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/gan_metrics/metric.py:213: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  fsmax = F.softmax(flogit)\n",
            "100%|██████████| 32/32 [00:10<00:00,  2.39it/s]\n",
            "  0%|          | 0/32 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting features...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:08<00:00,  3.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "compute score in space: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuFF4h_o9WL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "6b08ce7b-a186-496c-aa20-27f6afca03e0"
      },
      "source": [
        "_download_file_locally(net_g_filename, parent_fid)\n",
        "netG.load_state_dict(torch.load(net_g_filename))\n",
        "\n",
        "s = metric.compute_score_raw(dataset_name, \n",
        "                             image_size, \n",
        "                             dataroot, \n",
        "                             sample_size, \n",
        "                             batch_size, \n",
        "                             saveFolder_r=save_folder_r, \n",
        "                             saveFolder_f=save_folder_f, \n",
        "                             netG=netG, \n",
        "                             nz=nz, \n",
        "                             conv_model='inception_v3', \n",
        "                             workers=num_workers)\n",
        "np.save(metrics_filename, s)    \n",
        "_delete_all_remote_files(metrics_filename, parent_fid=parent_fid)\n",
        "gdrive.upload_file_to_folder(metrics_filename, parent_fid=parent_fid)\n",
        "# logging.getLogger().setLevel(logging.INFO)\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-08 10:37:48,113 1762 140435581867904: Submitting query 'name contains \"netG_epoch0024.pth\" and \"17HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe\" in parents and trashed = false'\n",
            "2019-05-08 10:37:48,121 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_epoch0024.pth%22+and+%2217HGmRF5jwfY2rTqzdKC6Ef-QtgvWvPAe%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-05-08 10:37:48,461 1762 140435581867904: URL being requested: GET https://www.googleapis.com/drive/v3/files/1eZe-52ESUeP5cCVGY7Cs5kDaLC66GUFd?alt=media\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-d69e0a28d7d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                              \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                              \u001b[0msaveFolder_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_folder_r\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                              \u001b[0msaveFolder_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_folder_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                              \u001b[0mnetG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_folder_r' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLPa8N_nAtX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "756004e1-5bea-4306-bbe2-c989d25a7fc0"
      },
      "source": [
        "metrics_filename"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('metrics_0024.npy',)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mNh6txYD8r-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "7c9b6311-0c84-4929-8610-750aa713c171"
      },
      "source": [
        "plot_rows = np.any(np.isfinite(score_tr), axis=1)\n",
        "assert any(plot_rows), \"No finite rows\"\n",
        "\n",
        "plot_score_tr = score_tr[plot_rows, :]\n",
        "plot_axis = np.arange(len(plot_rows))[plot_rows] \n",
        "\n",
        "num_metrics = len(metric_names)\n",
        "\n",
        "metrics_np = np.full((num_metrics, ), np.nan)\n",
        "for idx in range(num_metrics):\n",
        "    metric_name = metric_names[idx]\n",
        "    metric_value = plot_score_tr[-1, idx]\n",
        "    metrics_np[idx] = metric_value\n",
        "\n",
        "metrics_pd = pd.Series(metrics_np, index=metric_names)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-69b91109dee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplot_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No finite rows\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_score_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplot_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplot_rows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: No finite rows"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2JCm-D8jD0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collate_epoch = 24\n",
        "load_epoch = max_iter - 1\n",
        "\n",
        "attachments = []\n",
        "\n",
        "scale = .5\n",
        "for idx in range(num_metrics):  # idx = 0\n",
        "    metric_name = metric_names[idx]\n",
        "    logger.info(\"Plotting {}\".format(metric_name))\n",
        "    fig = plt.figure(figsize=(12 * scale, 4 * scale))\n",
        "    plt.plot(plot_axis, plot_score_tr[:, idx])\n",
        "    plt.title(metric_name)\n",
        "    ident = \"fig{:05d}\".format(idx)\n",
        "    fig_path = plotting.smart_save_fig(fig, ident=ident)\n",
        "    plt.close(fig)\n",
        "    attachments.append(fig_path)\n",
        "    \n",
        "real_filename = os.path.join(out_folder_local, 'real_samples.png')\n",
        "fake_filename = os.path.join(out_folder_local, 'fake_samples_epoch_{:03d}.png'.format(load_epoch))\n",
        "\n",
        "net_g_fullfilename = checkpoint_pattern.format(net_g_pattern, load_epoch)\n",
        "net_d_fullfilename = checkpoint_pattern.format(net_d_pattern, load_epoch)\n",
        "    \n",
        "storage_g_mb = os.path.getsize(net_g_fullfilename) / 1e6\n",
        "storage_d_mb = os.path.getsize(net_d_fullfilename) / 1e6\n",
        "\n",
        "flops_g, params_g = thop.profile(netG, input_size=(16, nz, 1, 1))\n",
        "flops_d, params_d = thop.profile(netD, input_size=(16, nc, 64, 64))\n",
        "\n",
        "lines = []\n",
        "lines += [\"Generator params, flops = {:.0f}, {:.0f}\".format(params_g, flops_g)]\n",
        "lines += [\"Discriminator params, flops = {:.0f}, {:.0f}\".format(flops_d, params_d)]\n",
        "lines += [\"Generator net storage (mb) = {:.0f}\".format(storage_g_mb)]\n",
        "lines += [\"Discriminator net storage (mb) = {:.0f}\".format(storage_d_mb)]\n",
        "lines += [\"Metrics after {} iterations ({}, seed = {})\".format(max_iter, identifier, prng_seed)]\n",
        "lines += [\"\\n\" + metrics_pd.to_string()]\n",
        "lines += [\"Real, then fake, images follow below\"]\n",
        "\n",
        "message = \"\\n\".join(lines)\n",
        "# logger.info(message)\n",
        "\n",
        "subject = 'GAN analysis results ({})'.format(identifier)\n",
        "\n",
        "attachments = [real_filename, fake_filename] + attachments\n",
        "send_email.send_mail_from_robotmatoba(to_addrs,\n",
        "                                      subject,\n",
        "                                      message, \n",
        "                                      attachments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsDsh-ISYU9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _generate_experiment_count_barplot(presence_pd: pd.DataFrame) -> str:  \n",
        "    tit = ''\n",
        "    xlabel = 'Completed Experiment Count'\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    labels = presence_pd.columns\n",
        "    num_labels = len(labels)\n",
        "    y_pos = np.arange(num_labels)\n",
        "    x_values = (presence_pd == 'o').sum()\n",
        "    error = None\n",
        "    ax.barh(y_pos,\n",
        "            x_values,\n",
        "            xerr=error,\n",
        "            align='center',\n",
        "            color='green',\n",
        "            ecolor='black')\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(labels)\n",
        "    ax.invert_yaxis()  # labels read top-to-bottom\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_title(tit)\n",
        "\n",
        "    plt.close(fig)\n",
        "    \n",
        "    experiment_counts_fig_path = plotting.smart_save_fig(fig, ident='experiment_counts')\n",
        "    return experiment_counts_fig_path\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PWVtgT0pZ0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _build_metrics_data(collate_epoch: int) -> pd.DataFrame:\n",
        "    logging.getLogger().setLevel(logging.ERROR)\n",
        "        \n",
        "    filename = metrics_pattern.format(collate_epoch)\n",
        "    idents_tuples = gdrive.list_dir(dataset_folder_fid)\n",
        "\n",
        "    num_idents = len(idents_tuples)\n",
        "\n",
        "    metrics = {k[0]: None for k in idents_tuples}\n",
        "    num_metrics = len(load_metrics)\n",
        "\n",
        "    for idx, ident_tuple in enumerate(idents_tuples):  # idx = 1; ident_tuple = idents_tuples[1]\n",
        "      \n",
        "        ident = ident_tuple[0]\n",
        "        ident_fid = ident_tuple[1]\n",
        "        logger.info(\"Loading from {}\".format(ident))\n",
        "\n",
        "        metrics_pattern.format(collate_epoch)\n",
        "\n",
        "        seed_tuples = gdrive.list_dir(ident_fid)\n",
        "        num_seeds = len(seed_tuples)\n",
        "        # logger.info(\"Found {} seeds\".format(num_seeds))\n",
        "\n",
        "        to_assign_np = np.full((num_seeds, num_metrics), np.nan)\n",
        "        seed_index = [None] * num_seeds  # np.full((num_seeds, ), np.nan)\n",
        "        for seed_idx, seed_tuple in enumerate(seed_tuples):\n",
        "            seed_identifier = seed_tuple[0]\n",
        "            seed_fid = seed_tuple[1]\n",
        "            seed_index[seed_idx] = seed_identifier\n",
        "            # logger.info(\"Downloading {}/{} to {}\".format(ident, seed_identifier))\n",
        "            metrics_filename = metrics_pattern.format(collate_epoch)\n",
        "\n",
        "            try:\n",
        "                _download_file_locally(metrics_filename, seed_fid)\n",
        "                # logger.info(\"Loading {} from {}\".format(metrics_filename, seed_fid))\n",
        "                s = np.load(metrics_filename)\n",
        "            except Exception as e:\n",
        "                s = None\n",
        "            s_pd = pd.Series(s, index=metric_names)\n",
        "            loaded_metrics = s_pd[load_metrics]\n",
        "            to_assign_np[seed_idx, :] = loaded_metrics.copy()\n",
        "\n",
        "        to_assign = pd.DataFrame(to_assign_np, \n",
        "                                 index=seed_index, \n",
        "                                 columns=load_metrics).sort_index(axis=0)      \n",
        "        metrics[ident] = to_assign\n",
        "    \n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "        \n",
        "    metrics_multiindex = pd.concat(metrics, axis=1, sort=True)\n",
        "    return metrics_multiindex\n",
        "\n",
        "\n",
        "  \n",
        "metrics_multiindex = _build_metrics_data(collate_epoch)\n",
        "\n",
        "presence_pd = pd.DataFrame('x', \n",
        "                           index=m_metric.index, \n",
        "                           columns=m_metric.columns).mask(np.isfinite(m_metric), 'o')\n",
        "\n",
        "experiment_counts_fig_path = _generate_experiment_count_barplot(presence_pd)\n",
        "\n",
        "lines = []\n",
        "lines += [presence_pd.to_html()]\n",
        "lines += [\"Total experiments\"]\n",
        "\n",
        "subject = \"Results completeness report\"\n",
        "message = \"\\n\".join(lines)\n",
        "# logger.info(message)\n",
        "attachments = [experiment_counts_fig_path]\n",
        "send_email.send_mail_from_robotmatoba(to_addrs,\n",
        "                                      subject,\n",
        "                                      message,                                       \n",
        "                                      attachments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URBlYUvGrpwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = []\n",
        "lines += [\"Means\"]\n",
        "lines += [metrics_multiindex.mean(axis=0).unstack().to_html()]\n",
        "lines += [\"Medians\"]\n",
        "lines += [metrics_multiindex.median(axis=0).unstack().to_html()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ywQRQIuXdG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attachments = []    \n",
        "for idx, metric_name in enumerate(load_metrics):\n",
        "    fig = plt.figure(figsize=(12 * scale, 4 * scale))\n",
        "    metrics_multiindex.xs(metric_name, axis=1, level=1).boxplot()\n",
        "    plt.title(metric_name)\n",
        "    ident = \"boxplot_{}\".format(metric_name)\n",
        "    fig_path = plotting.smart_save_fig(fig, ident=ident)\n",
        "    plt.close(fig)\n",
        "    attachments.append(fig_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5thhEAkVqxUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subject = \"Box plots\"\n",
        "message = \"\\n\".join(lines)\n",
        "\n",
        "attachments = attachments\n",
        "send_email.send_mail_from_robotmatoba(to_addrs,\n",
        "                                      subject,\n",
        "                                      message,                                       \n",
        "                                      attachments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiiDugNNfjqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filename = 'fake_samples_epoch_001.png'\n",
        "# filename = real_filename\n",
        "filename = fake_filename\n",
        "img = matplotlib.image.imread(filename)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}