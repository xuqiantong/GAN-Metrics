{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan_pytorch2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylematoba/GAN-Metrics/blob/master/dcgan_pytorch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hfsI5MSHQSJ3",
        "colab_type": "code",
        "outputId": "a593220b-6631-4bb5-f9cd-35b489f3b8c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf gan_metrics\n",
        "!git clone https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/GAN-Metrics.git gan_metrics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gan_metrics'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/20)   \u001b[K\rremote: Counting objects:  10% (2/20)   \u001b[K\rremote: Counting objects:  15% (3/20)   \u001b[K\rremote: Counting objects:  20% (4/20)   \u001b[K\rremote: Counting objects:  25% (5/20)   \u001b[K\rremote: Counting objects:  30% (6/20)   \u001b[K\rremote: Counting objects:  35% (7/20)   \u001b[K\rremote: Counting objects:  40% (8/20)   \u001b[K\rremote: Counting objects:  45% (9/20)   \u001b[K\rremote: Counting objects:  50% (10/20)   \u001b[K\rremote: Counting objects:  55% (11/20)   \u001b[K\rremote: Counting objects:  60% (12/20)   \u001b[K\rremote: Counting objects:  65% (13/20)   \u001b[K\rremote: Counting objects:  70% (14/20)   \u001b[K\rremote: Counting objects:  75% (15/20)   \u001b[K\rremote: Counting objects:  80% (16/20)   \u001b[K\rremote: Counting objects:  85% (17/20)   \u001b[K\rremote: Counting objects:  90% (18/20)   \u001b[K\rremote: Counting objects:  95% (19/20)   \u001b[K\rremote: Counting objects: 100% (20/20)   \u001b[K\rremote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 181 (delta 7), reused 1 (delta 0), pack-reused 161\u001b[K\n",
            "Receiving objects: 100% (181/181), 47.63 MiB | 36.67 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hvH7qnpmTI5A",
        "colab_type": "code",
        "outputId": "7f5c13db-c755-4577-b488-e37d1c3cd921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kylematoba/examples.git\n",
        "!git -C examples log -n 2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'examples' already exists and is not an empty directory.\n",
            "\u001b[33mcommit 73626a839f0ab7c7458dcde5c1a438bda0757fd9\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m\n",
            "Author: kylematoba <km3227@columbia.edu>\n",
            "Date:   Sat Apr 6 22:27:02 2019 +0100\n",
            "\n",
            "    Created using Colaboratory\n",
            "\n",
            "\u001b[33mcommit 5e91a5d17b2976cf95600cd25f658d469eeab84d\u001b[m\n",
            "Author: kylematoba <km3227@columbia.edu>\n",
            "Date:   Wed Apr 3 16:15:31 2019 +0100\n",
            "\n",
            "    Created using Colaboratory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gwSHE_dQZg2U",
        "colab_type": "code",
        "outputId": "1585dc82-9f1c-4acd-d0c7-f9e7ec3c417d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "# https://github.community/t5/How-to-use-Git-and-GitHub/Clone-private-repo/td-p/12616\n",
        "!rm -rf matoba_utils\n",
        "!git clone https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/matoba_utils.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'matoba_utils'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/13)   \u001b[K\rremote: Counting objects:  15% (2/13)   \u001b[K\rremote: Counting objects:  23% (3/13)   \u001b[K\rremote: Counting objects:  30% (4/13)   \u001b[K\rremote: Counting objects:  38% (5/13)   \u001b[K\rremote: Counting objects:  46% (6/13)   \u001b[K\rremote: Counting objects:  53% (7/13)   \u001b[K\rremote: Counting objects:  61% (8/13)   \u001b[K\rremote: Counting objects:  69% (9/13)   \u001b[K\rremote: Counting objects:  76% (10/13)   \u001b[K\rremote: Counting objects:  84% (11/13)   \u001b[K\rremote: Counting objects:  92% (12/13)   \u001b[K\rremote: Counting objects: 100% (13/13)   \u001b[K\rremote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects:  10% (1/10)   \u001b[K\rremote: Compressing objects:  20% (2/10)   \u001b[K\rremote: Compressing objects:  30% (3/10)   \u001b[K\rremote: Compressing objects:  40% (4/10)   \u001b[K\rremote: Compressing objects:  50% (5/10)   \u001b[K\rremote: Compressing objects:  60% (6/10)   \u001b[K\rremote: Compressing objects:  70% (7/10)   \u001b[K\rremote: Compressing objects:  80% (8/10)   \u001b[K\rremote: Compressing objects:  90% (9/10)   \u001b[K\rremote: Compressing objects: 100% (10/10)   \u001b[K\rremote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 13 (delta 3), reused 12 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:   7% (1/13)   \rUnpacking objects:  15% (2/13)   \rUnpacking objects:  23% (3/13)   \rUnpacking objects:  30% (4/13)   \rUnpacking objects:  38% (5/13)   \rUnpacking objects:  46% (6/13)   \rUnpacking objects:  53% (7/13)   \rUnpacking objects:  61% (8/13)   \rUnpacking objects:  69% (9/13)   \rUnpacking objects:  76% (10/13)   \rUnpacking objects:  84% (11/13)   \rUnpacking objects:  92% (12/13)   \rUnpacking objects: 100% (13/13)   \rUnpacking objects: 100% (13/13), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gT5mwRBMOrt-",
        "colab_type": "code",
        "outputId": "17e3e099-ae1a-4d8b-bd88-ab1338e5aaf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install pot"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pot in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pot) (1.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pot) (1.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pot) (3.0.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from pot) (0.29.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->pot) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->pot) (40.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tdnOpHY1kl7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import sys\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import gan_metrics.metric as metric\n",
        "\n",
        "FORMAT = \"%(asctime)s %(process)s %(thread)s: %(message)s\"\n",
        "logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "dict_environ = dict(os.environ)\n",
        "# logger.info(pprint.pformat(dict_environ, indent=4))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "irLFxc4dkpKu",
        "colab_type": "code",
        "outputId": "9291ad56-2847-4929-a1aa-81cbc89a6141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dataroot = 'examples'\n",
        "# prng_seed = None\n",
        "prng_seed = 9\n",
        "# prng_seed = 8\n",
        "# prng_seed = 1\n",
        "# prng_seed = 10\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "is_cuda = True\n",
        "lr = 0.0002\n",
        "beta1 = .5\n",
        "\n",
        "# max_iter = 25\n",
        "max_iter = 50\n",
        "# max_iter = 3\n",
        "# max_iter = 10\n",
        "num_workers = 2\n",
        "ngpu = 1\n",
        "# nz = 100\n",
        "nz = 50\n",
        "# nz = 200\n",
        "\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "if prng_seed is None:\n",
        "    prng_seed = random.randint(1, 10000)\n",
        "\n",
        "# assert prng_seed < 1000, \"Not supporting seeds with more than 5 digits\"\n",
        "# identifier = 'ident'\n",
        "# identifier_base = 'z{:05d}seed{:05d}'.format(nz, prng_seed)\n",
        "identifier_base = 'z{:05d}'.format(nz)\n",
        "out_folder = '/content'\n",
        "\n",
        "# dataset_name = 'lsun'\n",
        "# dataset_name = 'mnist'\n",
        "dataset_name = 'cifar10'\n",
        "# identifier = identifier_base + '_' + dataset_name + '_seed' + str(prng_seed)\n",
        "# identifier = \"{}_{}_seed{:04d}\".format(identifier_base, dataset_name, prng_seed)\n",
        "identifier = identifier_base + '_' + dataset_name\n",
        "\n",
        "print_every_iteration = 200\n",
        "save_every_iteration = 200\n",
        "checkpoint_every_epoch = 2\n",
        "\n",
        "logger.info(\"Identifier: {}\".format(identifier))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-21 19:10:43,602 277 140241219331968: Identifier: z00050_cifar10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cxciv29hTna5",
        "colab_type": "code",
        "outputId": "1bb5a8bc-6d82-4acc-8729-9330d96d23da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "logger.info(\"Random Seed: {}\".format(prng_seed))\n",
        "random.seed(prng_seed)\n",
        "torch.manual_seed(prng_seed)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if torch.cuda.is_available() and not is_cuda:\n",
        "    logger.info(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "if dataset_name in ['imagenet', 'folder', 'lfw']:\n",
        "    # folder dataset\n",
        "    dataset = dset.ImageFolder(root=dataroot,\n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.Resize(image_size),\n",
        "                                   transforms.CenterCrop(image_size),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                               ]))\n",
        "    nc=3\n",
        "elif dataset_name == 'lsun':\n",
        "    dataset = dset.LSUN(root=dataroot, classes=['bedroom_train'],\n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.Resize(image_size),\n",
        "                            transforms.CenterCrop(image_size),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                        ]))\n",
        "    nc=3\n",
        "elif dataset_name == 'cifar10':\n",
        "    dataset = dset.CIFAR10(root=dataroot, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "    nc=3\n",
        "\n",
        "elif dataset_name == 'mnist':\n",
        "        dataset = dset.MNIST(root=dataroot, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5,), (0.5,)),\n",
        "                           ]))\n",
        "        nc=1\n",
        "\n",
        "elif dataset_name == 'fake':\n",
        "    dataset = dset.FakeData(image_size=(3, image_size, image_size),\n",
        "                            transform=transforms.ToTensor())\n",
        "    nc=3\n",
        "\n",
        "assert dataset\n",
        "device = torch.device(\"cuda:0\" if is_cuda else \"cpu\")\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu: int):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu: int):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "netD.apply(weights_init)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-21 19:10:43,710 277 140241219331968: Random Seed: 9\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (12): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "5zx1BHJhM6qK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matoba_utils.gdrive as gdrive\n",
        "\n",
        "\n",
        "def _delete_all_remote_files(del_filename: str) -> None:\n",
        "    del_files = gdrive.find_items(name=del_filename)\n",
        "    for x in del_files:\n",
        "        logger.info(\"Deleting {}\".format(x))\n",
        "        gdrive.delete_file(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6r8HTg7ULvRj",
        "colab_type": "code",
        "outputId": "744a95df-950e-4943-98db-9f6b098d3ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "cell_type": "code",
      "source": [
        "# Check whether there are checkpoints in the google drive\n",
        "username = 'robotmatoba'\n",
        "gdrive.authenticate_automatically(username)\n",
        "checkpoint_dir = out_folder\n",
        "\n",
        "folder_name = 'PytorchCheckpoints'\n",
        "found_folders = gdrive.find_items(folder_name)\n",
        "print(found_folders)\n",
        "if len(found_folders) > 0:\n",
        "    parent_fid = found_folders[0]\n",
        "else:\n",
        "    parent_fid = gdrive.create_folder(folder_name)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-21 19:10:49,074 277 140241219331968: file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "2019-04-21 19:10:49,076 277 140241219331968: URL being requested: GET https://www.googleapis.com/discovery/v1/apis/drive/v3/rest\n",
            "2019-04-21 19:10:49,149 277 140241219331968: No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
            "2019-04-21 19:10:49,159 277 140241219331968: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22PytorchCheckpoints%22and+trashed+%3D+false&alt=json\n",
            "[]\n",
            "2019-04-21 19:10:49,387 277 140241219331968: URL being requested: POST https://www.googleapis.com/drive/v3/files?fields=id&alt=json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0JKJKMiQz01p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net_g_pattern = 'netG_{}_epoch'.format(identifier)\n",
        "net_d_pattern = 'netD_{}_epoch'.format(identifier)\n",
        "checkpoint_pattern = '{}{:04d}.pth'\n",
        "\n",
        "# net_g_path = os.path.join(checkpoint_dir, identifier, dataset_name, \"g\")\n",
        "# net_d_path = os.path.join(checkpoint_dir, identifier, dataset_name, \"d\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onmQ-Nd3bXFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saveFolder_r = os.path.join(out_folder, 'real/')\n",
        "saveFolder_f = os.path.join(out_folder, 'fake/')\n",
        "\n",
        "os.makedirs(saveFolder_r, exist_ok=True)\n",
        "os.makedirs(saveFolder_f, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GTMbS2qhPE2R",
        "colab_type": "code",
        "outputId": "e30bb919-122d-4b46-8c37-e924b29a008d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "net_g_items = gdrive.find_items(name=net_g_pattern)\n",
        "net_d_items = gdrive.find_items(name=net_d_pattern)\n",
        "sorted_net_g_filenames = sorted([x.name for x in net_g_items])\n",
        "sorted_net_d_filenames = sorted([x.name for x in net_d_items])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-21 19:10:49,752 277 140241219331968: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_z00050_cifar10_epoch%22and+trashed+%3D+false&alt=json\n",
            "2019-04-21 19:10:49,929 277 140241219331968: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_z00050_cifar10_epoch%22and+trashed+%3D+false&alt=json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hb7m463KKNZo",
        "colab_type": "code",
        "outputId": "1842626c-2e42-4815-b4b1-f6c3a03b4ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "net_g_pattern\n",
        "print(net_g_pattern)\n",
        "gdrive.find_items(name=net_g_pattern)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "netG_z00050_cifar10_epoch\n",
            "2019-04-21 19:10:50,113 277 140241219331968: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_z00050_cifar10_epoch%22and+trashed+%3D+false&alt=json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "vaX2RRDasY0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "attempt_reload = True\n",
        "# attempt_reload = False\n",
        "# max_load = 0\n",
        "max_load = None\n",
        "\n",
        "\n",
        "def _get_epoch_from_checkpoint(x: str) -> int:\n",
        "    return int(x.rstrip('.pth').split('epoch')[-1])\n",
        "  \n",
        "  \n",
        "def _download_file_locally(filestr: str) -> None:\n",
        "    remote_files = gdrive.find_items(name=filestr)\n",
        "    assert 1 == len(remote_files), str(remote_files)\n",
        "    remote_file = remote_files[0]\n",
        "    gdrive.download_file_to_folder(remote_file, filestr)\n",
        "\n",
        "\n",
        "if attempt_reload and len(sorted_net_g_filenames) > 2 and len(sorted_net_d_filenames) > 2:\n",
        "    latest_net_g_filename = max(sorted_net_g_filenames)\n",
        "    latest_net_d_filename = max(sorted_net_d_filenames)\n",
        "\n",
        "    latest_net_g_epoch = _get_epoch_from_checkpoint(latest_net_g_filename)\n",
        "    latest_net_d_epoch = _get_epoch_from_checkpoint(latest_net_d_filename)\n",
        "\n",
        "    latest_epoch = min(latest_net_g_epoch, latest_net_d_epoch)\n",
        "\n",
        "    net_g_filename = checkpoint_pattern.format(net_g_pattern, latest_epoch)\n",
        "    net_d_filename = checkpoint_pattern.format(net_d_pattern, latest_epoch)\n",
        "\n",
        "    last_net_g_fullfilename = sorted_net_g_filenames[sorted_net_g_filenames.index(net_g_filename) - 1]\n",
        "    last_net_d_fullfilename = sorted_net_d_filenames[sorted_net_d_filenames.index(net_d_filename) - 1]\n",
        "\n",
        "    g_epoch = _get_epoch_from_checkpoint(last_net_g_fullfilename)\n",
        "    d_epoch = _get_epoch_from_checkpoint(last_net_d_fullfilename)\n",
        "\n",
        "    load_epoch = min(d_epoch, g_epoch)\n",
        "    if max_load is not None:\n",
        "       load_epoch = min(load_epoch, max_load)\n",
        "        \n",
        "    net_g_fullfilename = checkpoint_pattern.format(net_g_pattern, load_epoch)\n",
        "    net_d_fullfilename = checkpoint_pattern.format(net_d_pattern, load_epoch)\n",
        "                \n",
        "    _download_file_locally(net_d_fullfilename)\n",
        "    _download_file_locally(net_g_fullfilename)\n",
        "    min_iter = load_epoch\n",
        "    logger.info(\"Loading from epoch {:04d}\".format(load_epoch))\n",
        "\n",
        "else:\n",
        "    net_g_fullfilename = ''\n",
        "    net_d_fullfilename = ''\n",
        "\n",
        "    min_iter = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hrcWGX_othO9",
        "colab_type": "code",
        "outputId": "06521e93-9996-44e3-f40e-09b827bacac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "cell_type": "code",
      "source": [
        "if net_g_fullfilename != '':\n",
        "    netG.load_state_dict(torch.load(net_g_fullfilename))\n",
        "\n",
        "if net_d_fullfilename != '':\n",
        "    netD.load_state_dict(torch.load(net_d_fullfilename))\n",
        "\n",
        "logger.info(netD)\n",
        "logger.info(netG)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    os.makedirs(out_folder, exist_ok=True)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    real_label = 1\n",
        "    fake_label = 0\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=num_workers)\n",
        "\n",
        "    # set up optimizer\n",
        "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "    dataloader_size = len(dataloader)\n",
        "    \n",
        "    logger.info(\"Running iterations {} to {}\".format(min_iter, max_iter))\n",
        "    \n",
        "    for epoch in range(min_iter, max_iter):\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "            # train with real\n",
        "            netD.zero_grad()\n",
        "            real_cpu = data[0].to(device)\n",
        "            batch_size = real_cpu.size(0)\n",
        "            label = torch.full((batch_size,), real_label, device=device)\n",
        "\n",
        "            output = netD(real_cpu)\n",
        "            errD_real = criterion(output, label)\n",
        "            errD_real.backward()\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "            # train with fake\n",
        "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "            fake = netG(noise)\n",
        "            label.fill_(fake_label)\n",
        "            output = netD(fake.detach())\n",
        "            errD_fake = criterion(output, label)\n",
        "            errD_fake.backward()\n",
        "            D_G_z1 = output.mean().item()\n",
        "            errD = errD_real + errD_fake\n",
        "            optimizerD.step()\n",
        "\n",
        "            # (2) Update G network: maximize log(D(G(z)))\n",
        "            netG.zero_grad()\n",
        "            label.fill_(real_label)  # fake labels are real for generator cost\n",
        "            output = netD(fake)\n",
        "            errG = criterion(output, label)\n",
        "            errG.backward()\n",
        "            D_G_z2 = output.mean().item()\n",
        "            optimizerG.step()\n",
        "\n",
        "            if i % print_every_iteration == 0:\n",
        "                loss_d = errD.item()\n",
        "                loss_g = errG.item()\n",
        "\n",
        "                logger.info('[%d/%d] [%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "                      % (epoch, max_iter, i, dataloader_size, loss_d, loss_g, D_x, D_G_z1, D_G_z2))\n",
        "            if i % save_every_iteration == 0:\n",
        "                real_filename = '%s/real_samples.png' % out_folder\n",
        "                fake_filename = '%s/fake_samples_epoch_%03d.png' % (out_folder, epoch)\n",
        "\n",
        "                fake = netG(fixed_noise)\n",
        "                \n",
        "                vutils.save_image(real_cpu, real_filename, normalize=True)\n",
        "                vutils.save_image(fake.detach(), fake_filename, normalize=True)\n",
        "                \n",
        "        net_g_filename = checkpoint_pattern.format(net_g_pattern, epoch)\n",
        "        net_d_filename = checkpoint_pattern.format(net_d_pattern, epoch)\n",
        "\n",
        "        net_g_full_filename = os.path.join(checkpoint_dir, net_g_filename)\n",
        "        net_d_full_filename = os.path.join(checkpoint_dir, net_d_filename)\n",
        "\n",
        "        torch.save(netG.state_dict(), net_g_full_filename)\n",
        "        torch.save(netD.state_dict(), net_d_full_filename)\n",
        "\n",
        "        if 0 == epoch % checkpoint_every_epoch:\n",
        "            logger.info('Checkpointing epoch {}'.format(epoch))\n",
        "\n",
        "            # Delete any existing files with this name, to avoid ending up with multiple files\n",
        "            _delete_all_remote_files(net_g_filename)\n",
        "            _delete_all_remote_files(net_d_filename)\n",
        "            \n",
        "            gdrive.upload_file_to_folder(net_g_filename, folder=parent_fid)\n",
        "            gdrive.upload_file_to_folder(net_d_filename, folder=parent_fid) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-21 19:10:50,422 277 140241219331968: Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n",
            "2019-04-21 19:10:50,424 277 140241219331968: Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(50, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n",
            "2019-04-21 19:10:50,427 277 140241219331968: Running iterations 0 to 50\n",
            "2019-04-21 19:10:50,955 277 140241219331968: [0/50] [0/782] Loss_D: 1.4440 Loss_G: 3.5941 D(x): 0.4502 D(G(z)): 0.3102 / 0.0334\n",
            "2019-04-21 19:11:05,916 277 140241219331968: [0/50] [200/782] Loss_D: 0.1290 Loss_G: 4.3831 D(x): 0.9505 D(G(z)): 0.0693 / 0.0165\n",
            "2019-04-21 19:11:20,923 277 140241219331968: [0/50] [400/782] Loss_D: 0.4586 Loss_G: 3.7256 D(x): 0.7112 D(G(z)): 0.0481 / 0.0339\n",
            "2019-04-21 19:11:35,491 277 140241219331968: [0/50] [600/782] Loss_D: 0.2947 Loss_G: 2.6681 D(x): 0.8481 D(G(z)): 0.1026 / 0.0900\n",
            "2019-04-21 19:11:48,624 277 140241219331968: Checkpointing epoch 0\n",
            "2019-04-21 19:11:48,627 277 140241219331968: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_z00050_cifar10_epoch0000.pth%22and+trashed+%3D+false&alt=json\n",
            "2019-04-21 19:11:48,772 277 140241219331968: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_z00050_cifar10_epoch0000.pth%22and+trashed+%3D+false&alt=json\n",
            "2019-04-21 19:11:48,926 277 140241219331968: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n",
            "2019-04-21 19:11:50,149 277 140241219331968: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n",
            "2019-04-21 19:11:52,818 277 140241219331968: [1/50] [0/782] Loss_D: 0.6884 Loss_G: 2.4515 D(x): 0.6138 D(G(z)): 0.0533 / 0.1330\n",
            "2019-04-21 19:12:07,118 277 140241219331968: [1/50] [200/782] Loss_D: 0.6789 Loss_G: 2.3250 D(x): 0.6270 D(G(z)): 0.0787 / 0.1531\n",
            "2019-04-21 19:12:21,628 277 140241219331968: [1/50] [400/782] Loss_D: 0.5244 Loss_G: 3.6858 D(x): 0.7305 D(G(z)): 0.1158 / 0.0477\n",
            "2019-04-21 19:12:36,339 277 140241219331968: [1/50] [600/782] Loss_D: 1.0232 Loss_G: 6.3920 D(x): 0.9079 D(G(z)): 0.5438 / 0.0032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EW9WIm2JvMy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metrics_pattern = \"metrics_{}\".format(identifier)\n",
        "sample_size = 2000\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "# [emd-mmd-knn(knn,real,fake,precision,recall)]*4 - IS - mode_score - FID\n",
        "score_tr = np.zeros((max_iter, 4*7+3))\n",
        "# max_iter = 5\n",
        "for epoch in range(0, max_iter):\n",
        "    metrics_filename = \"{}_{:04d}.npy\".format(metrics_pattern, epoch)\n",
        "    logger.info(metrics_filename)\n",
        "    found_items = gdrive.find_items(name=metrics_filename)\n",
        "    \n",
        "    if len(found_items) > 0:\n",
        "        assert len(found_items) <= 1\n",
        "        found_item = found_items[0]\n",
        "        assert found_item.name == metrics_filename\n",
        "        _download_file_locally(metrics_filename)\n",
        "        s = np.load(metrics_filename)\n",
        "    else:\n",
        "        net_g_filename = checkpoint_pattern.format(net_g_pattern, epoch)\n",
        "\n",
        "        logger.info(\"Downloading {}\".format(net_g_filename))\n",
        "        try:\n",
        "            _download_file_locally(net_g_filename)\n",
        "            netG.load_state_dict(torch.load(net_g_filename))\n",
        "            logger.info(\"Computing metrics on {}\".format(net_g_filename))\n",
        "\n",
        "            s = metric.compute_score_raw(dataset_name, \n",
        "                                         image_size, \n",
        "                                         dataroot, \n",
        "                                         sample_size, \n",
        "                                         batch_size, \n",
        "                                         saveFolder_r=saveFolder_r, \n",
        "                                         saveFolder_f=saveFolder_f, \n",
        "                                         netG=netG, \n",
        "                                         nz=nz, \n",
        "                                         conv_model='inception_v3', \n",
        "                                         workers=num_workers)\n",
        "            np.save(metrics_filename, s)    \n",
        "            _delete_all_remote_files(metrics_filename)\n",
        "            gdrive.upload_file_to_folder(metrics_filename, folder=parent_fid)\n",
        "        except: \n",
        "          s = np.nan\n",
        "    score_tr[epoch, :] = s\n",
        "      \n",
        "# # save final metric scores of all epoches\n",
        "# save_fn = '%s/score_tr_ep.npy' % out_folder\n",
        "# np.save(save_fn, score_tr)\n",
        "# logger.info('##### training completed#####')\n",
        "# logger.info('### metric scores output is scored at {} ###'.format(save_fn))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j-q7ZeEnBW4t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_rows = np.any(np.isfinite(score_tr), axis=1)\n",
        "plot_score_tr = score_tr[plot_rows, :]\n",
        "plot_axis = np.arange(len(plot_rows))[plot_rows] \n",
        "# print(plot_score_tr)\n",
        "# print(plot_axis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yctPegCoY-2M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metric_names = np.array(['pixl_wasserstein', 'pixl_mmd', 'pixl_acc', 'pixl_acc_t',\n",
        "                         'pixl_acc_f', 'pixl_precision', 'pixl_recall', 'conv_wasserstein',\n",
        "                         'conv_mmd', 'conv_acc', 'conv_acc_t', 'conv_acc_f',\n",
        "                         'conv_precision', 'conv_recall', 'logit_wasserstein', 'logit_mmd',\n",
        "                         'logit_acc', 'logit_acc_t', 'logit_acc_f', 'logit_precision',\n",
        "                         'logit_recall', 'smax_wasserstein', 'smax_mmd', 'smax_acc',\n",
        "                         'smax_acc_t', 'smax_acc_f', 'smax_precision', 'smax_recall',\n",
        "                         'inception_score', 'mode_score', 'fid'], dtype=object)\n",
        "\n",
        "num_metrics = len(metric_names)\n",
        "\n",
        "num_cols = 3\n",
        "num_rows = int(np.ceil(num_metrics / num_cols))\n",
        "fig, ax = plt.subplots(num_rows, num_cols, figsize=(20,10))\n",
        "logger.info(\"Plotting results\")\n",
        "# for idx, metric in enumerate(metric_names):\n",
        "for row in range(num_rows):\n",
        "    for col in range(num_cols):\n",
        "        idx = row * num_cols + col\n",
        "        if idx >= num_metrics:\n",
        "            continue\n",
        "        a_ = ax[row, col]\n",
        "        # plt.subplot(num_rows, num_cols, idx)\n",
        "        a_.plot(plot_axis, plot_score_tr[:, idx])\n",
        "        a_.set_title(metric_names[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1mNh6txYD8r-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "metrics_np = np.full((num_metrics, ), np.nan)\n",
        "for idx in range(num_metrics):\n",
        "    metric_name = metric_names[idx]\n",
        "    metric_value = plot_score_tr[-1, idx]\n",
        "    metrics_np[idx] = metric_value\n",
        "    # logger.info(\"{:25s}: {:+5.4f}\".format(metric_name, metric_value))\n",
        "    \n",
        "import pandas as pd\n",
        "\n",
        "metrics_pd = pd.Series(metrics_np, index=metric_names)\n",
        "\n",
        "logger.info(\"Metrics after {} iterations ({}, seed = {})\".format(max_iter, identifier, prng_seed))\n",
        "logger.info(\"\\n\" + metrics_pd.to_string())\n",
        "\n",
        "lines = []\n",
        "lines += [\"Metrics after {} iterations ({}, seed = {})\".format(max_iter, identifier, prng_seed)]\n",
        "lines += [\"\\n\" + metrics_pd.to_string()]\n",
        "\n",
        "message = \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "\n",
        "to_addrs = ['kylematoba@gmail.com']\n",
        "subject = 'GAN analysis results ({identifier})'.format(identifier=identifier)\n",
        "# logger.info(subject)\n",
        "\n",
        "import matoba_utils.send_email as send_email\n",
        "send_email.send_mail_from_robotmatoba(to_addrs,\n",
        "                                      subject,\n",
        "                                      message)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uz9G2v_Vf9Xm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "net_g_filename = checkpoint_pattern.format(net_g_pattern, epoch)\n",
        "\n",
        "logger.info(\"Downloading {}\".format(net_g_filename))\n",
        "netG.load_state_dict(torch.load(net_g_filename))\n",
        "logger.info(\"Computing metrics on {}\".format(g_filename))\n",
        "\n",
        "s = metric.compute_score_raw(dataset_name, \n",
        "                             image_size, \n",
        "                             dataroot, \n",
        "                             sample_size, \n",
        "                             batch_size, \n",
        "                             saveFolder_r=saveFolder_r, \n",
        "                             saveFolder_f=saveFolder_f, \n",
        "                             netG=netG, \n",
        "                             nz=nz, \n",
        "                             conv_model='inception_v3', \n",
        "                             workers=num_workers)\n",
        "np.save(metrics_filename, s)    \n",
        "_delete_all_remote_files(metrics_filename)\n",
        "gdrive.upload_file_to_folder(metrics_filename, folder=parent_fid)    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H-E0qYcawu5i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJ6rWgMge4-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !ls MNIST_DCGAN_results/Fixed_results/\n",
        "# !ls MNIST_DCGAN_results/Random_results/\n",
        "# !ls\n",
        "# os.path.split(g_filename)[1]\n",
        "!ls -ltra\n",
        "# net_g_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiiDugNNfjqg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# filename = 'fake_samples_epoch_001.png'\n",
        "filename = 'fake_samples_epoch_016.png'\n",
        "img = matplotlib.image.imread(filename)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PKIyTqHoUTDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# https://github.com/kylematoba/GAN-Metrics/blob/master/metric.py#L399\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}