{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan_pytorch2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylematoba/GAN-Metrics/blob/master/dcgan_pytorch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hvH7qnpmTI5A",
        "colab_type": "code",
        "outputId": "414364ae-6b0c-4582-df44-2c8c3ddcd6a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kylematoba/examples.git\n",
        "!git -C examples log -n 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'examples' already exists and is not an empty directory.\n",
            "\u001b[33mcommit 73626a839f0ab7c7458dcde5c1a438bda0757fd9\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m\n",
            "Author: kylematoba <km3227@columbia.edu>\n",
            "Date:   Sat Apr 6 22:27:02 2019 +0100\n",
            "\n",
            "    Created using Colaboratory\n",
            "\n",
            "\u001b[33mcommit 5e91a5d17b2976cf95600cd25f658d469eeab84d\u001b[m\n",
            "Author: kylematoba <km3227@columbia.edu>\n",
            "Date:   Wed Apr 3 16:15:31 2019 +0100\n",
            "\n",
            "    Created using Colaboratory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gwSHE_dQZg2U",
        "colab_type": "code",
        "outputId": "39a4c7ed-d246-4e38-d2f6-e6ebbea580eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "# https://github.community/t5/How-to-use-Git-and-GitHub/Clone-private-repo/td-p/12616\n",
        "!rm -rf gdrive_checkpoint\n",
        "!git clone https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/gdrive_checkpoint.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gdrive_checkpoint'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 33 (delta 16), reused 28 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hfsI5MSHQSJ3",
        "colab_type": "code",
        "outputId": "8f14c279-4ca2-41f8-cc64-aab976e8f135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf gan_metrics\n",
        "!git clone https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/GAN-Metrics.git gan_metrics"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gan_metrics'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 172 (delta 2), reused 1 (delta 0), pack-reused 161\u001b[K\n",
            "Receiving objects: 100% (172/172), 47.45 MiB | 9.74 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gT5mwRBMOrt-",
        "colab_type": "code",
        "outputId": "38400abe-95ec-4c2b-907e-2e5a22bde026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install pot"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pot in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from pot) (0.29.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pot) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pot) (1.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pot) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (2.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->pot) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->pot) (40.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RrbM57OKcbUJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tdnOpHY1kl7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import sys\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import gan_metrics.metric as metric\n",
        "\n",
        "FORMAT = \"%(asctime)s %(process)s %(thread)s: %(message)s\"\n",
        "logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "dict_environ = dict(os.environ)\n",
        "# logger.info(pprint.pformat(dict_environ, indent=4))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "irLFxc4dkpKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# identifier = 'ident'\n",
        "identifier_base = 'z100'\n",
        "out_folder = '/content'\n",
        "# dataset_name = 'mnist'\n",
        "dataset_name = 'cifar10'\n",
        "identifier = identifier_base + '_' + dataset_name\n",
        "\n",
        "dataroot = 'examples'\n",
        "prng_seed = None\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "is_cuda = True\n",
        "lr = 0.0002\n",
        "beta1 = .5\n",
        "\n",
        "# max_iter = 25\n",
        "max_iter = 4\n",
        "num_workers = 2\n",
        "ngpu = 1\n",
        "nz = 100\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "\n",
        "print_every_iteration = 100\n",
        "save_every_iteration = 200\n",
        "checkpoint_every_epoch = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cxciv29hTna5",
        "colab_type": "code",
        "outputId": "fce96116-417d-4dac-8115-edb608f6bf86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "if prng_seed is None:\n",
        "    prng_seed = random.randint(1, 10000)\n",
        "\n",
        "logger.info(\"Random Seed: {}\".format(prng_seed))\n",
        "random.seed(prng_seed)\n",
        "torch.manual_seed(prng_seed)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if torch.cuda.is_available() and not is_cuda:\n",
        "    logger.info(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "if dataset_name in ['imagenet', 'folder', 'lfw']:\n",
        "    # folder dataset\n",
        "    dataset = dset.ImageFolder(root=dataroot,\n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.Resize(image_size),\n",
        "                                   transforms.CenterCrop(image_size),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                               ]))\n",
        "    nc=3\n",
        "elif dataset_name == 'lsun':\n",
        "    dataset = dset.LSUN(root=dataroot, classes=['bedroom_train'],\n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.Resize(image_size),\n",
        "                            transforms.CenterCrop(image_size),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                        ]))\n",
        "    nc=3\n",
        "elif dataset_name == 'cifar10':\n",
        "    dataset = dset.CIFAR10(root=dataroot, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "    nc=3\n",
        "\n",
        "elif dataset_name == 'mnist':\n",
        "        dataset = dset.MNIST(root=dataroot, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5,), (0.5,)),\n",
        "                           ]))\n",
        "        nc=1\n",
        "\n",
        "elif dataset_name == 'fake':\n",
        "    dataset = dset.FakeData(image_size=(3, image_size, image_size),\n",
        "                            transform=transforms.ToTensor())\n",
        "    nc=3\n",
        "\n",
        "assert dataset\n",
        "device = torch.device(\"cuda:0\" if is_cuda else \"cpu\")\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu: int):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu: int):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "netD.apply(weights_init)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-14 15:44:45,287 1145 140017142212480: Random Seed: 3858\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (12): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "5zx1BHJhM6qK",
        "colab_type": "code",
        "outputId": "fef65f0b-5ae3-4f79-8ca5-a84a920e7d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "cell_type": "code",
      "source": [
        "import gdrive_checkpoint.core\n",
        "import gdrive_checkpoint.utils\n",
        "\n",
        "gdrive_checkpoint.core.authenticate_automatically()\n",
        "# ds = gdrive_checkpoint.core.build_drive_service()\n",
        "\n",
        "def _delete_all_remote_files(del_filename: str) -> None:\n",
        "    del_files = gdrive_checkpoint.core.find_items(name=del_filename)\n",
        "    for x in del_files:\n",
        "        logger.info(\"Deleting {}\".format(x))\n",
        "        gdrive_checkpoint.core.delete_file(x)\n",
        "\n",
        "\n",
        "# del_filename = 'netD_z100_cifar10_epoch_0022.pth'\n",
        "# _delete_all_remote_files(del_filename)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-14 15:44:49,332 1145 140017142212480: file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "2019-04-14 15:44:49,334 1145 140017142212480: URL being requested: GET https://www.googleapis.com/discovery/v1/apis/drive/v3/rest\n",
            "2019-04-14 15:44:49,354 1145 140017142212480: No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
            "2019-04-14 15:44:49,364 1145 140017142212480: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_z100_cifar10_epoch_0022.pth%22and+trashed+%3D+false&alt=json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wC5Ew---0TwD",
        "colab_type": "code",
        "outputId": "c935b813-c354-4bb8-b168-e6a6a9a9a568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "name = 'abc'\n",
        "drive_service = gdrive_checkpoint.core.build_drive_service()\n",
        "list_str = 'name contains \"%s\" and trashed=false' % name\n",
        "trashed = False\n",
        "\n",
        "folder_list = drive_service.files().list(q=list_str).execute()\n",
        "print(folder_list)\n",
        "# folders = []\n",
        "# for folder in folder_list['files']:\n",
        "#     folders.append(GDriveItem(folder['name'], folder['id']))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-14 15:44:49,763 1145 140017142212480: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22abc%22+and+trashed%3Dfalse&alt=json\n",
            "{'kind': 'drive#fileList', 'incompleteSearch': False, 'files': []}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6r8HTg7ULvRj",
        "colab_type": "code",
        "outputId": "d8aa75d7-dc17-4833-9753-2e5000aee35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Check whether there are checkpoints in the google drive\n",
        "gdrive_checkpoint.core.authenticate_automatically()\n",
        "checkpoint_dir = out_folder\n",
        "\n",
        "folder_name = 'PytorchCheckpoints'\n",
        "found_folders = gdrive_checkpoint.core.find_items(folder_name)\n",
        "print(found_folders)\n",
        "if len(found_folders) > 0:\n",
        "    parent_fid = found_folders[0]\n",
        "else:\n",
        "    parent_fid = gdrive_checkpoint.core.create_folder(folder_name)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-14 15:44:50,152 1145 140017142212480: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22PytorchCheckpoints%22and+trashed+%3D+false&alt=json\n",
            "[GDriveItem(name='PytorchCheckpoints', fid='1C7cZgTWt9R_Hmy0bkHPrCRKUxr58qrLX')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U8PUpSmh2rX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# drive_service = gdrive_checkpoint.core.build_drive_service()\n",
        "# list_str = 'name contains \"%s\"' % name\n",
        "# folder_list = drive_service.files().list(q=list_str).execute()\n",
        "# folders = []\n",
        "# for folder in folder_list['files']:\n",
        "#     folders.append(GDriveItem(folder['name'], folder['id']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EL4NB1_tz3P1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # https://developers.google.com/drive/api/v3/folder\n",
        "# file_metadata = {\n",
        "#     'name': folder_name,\n",
        "#     'mimeType': 'application/vnd.google-apps.folder'\n",
        "# }\n",
        "# # drive_service = build_drive_service()\n",
        "\n",
        "# drive_service = gdrive_checkpoint.core.build_drive_service()\n",
        "# file = drive_service.files().create(body=file_metadata,\n",
        "#                                     fields='id').execute()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yciTpuZF6Tjx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# logger.info(gdrive_checkpoint.core.GDriveItem(folder_name, file['id']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0JKJKMiQz01p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g_pattern = 'netG_{}_epoch'.format(identifier)\n",
        "d_pattern = 'netD_{}_epoch'.format(identifier)\n",
        "checkpoint_pattern = '{}_{:04d}.pth'\n",
        "\n",
        "# net_g_path = os.path.join(checkpoint_dir, identifier, dataset_name, \"g\")\n",
        "# net_d_path = os.path.join(checkpoint_dir, identifier, dataset_name, \"d\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LTJoV9LryhIf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# drive_service = gdrive_checkpoint.core.build_drive_service()\n",
        "# print(drive_service)\n",
        "# file_metadata = {\n",
        "#     'name': folder_name,\n",
        "#     'mimeType': 'application/vnd.google-apps.folder'\n",
        "# }\n",
        "# file = drive_service.files().create(body=file_metadata,\n",
        "#                                     fields='id').execute()\n",
        "# logger.info(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onmQ-Nd3bXFJ",
        "colab_type": "code",
        "outputId": "2281cb49-6319-44de-e6ab-0328712a3a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# [emd-mmd-knn(knn,real,fake,precision,recall)]*4 - IS - mode_score - FID\n",
        "score_tr = np.zeros((max_iter, 4*7+3))\n",
        "\n",
        "sample_size = 2000\n",
        "\n",
        "saveFolder_r = os.path.join(out_folder, 'real/')\n",
        "saveFolder_f = os.path.join(out_folder, 'fake/')\n",
        "\n",
        "os.makedirs(saveFolder_r, exist_ok=True)\n",
        "os.makedirs(saveFolder_f, exist_ok=True)\n",
        "\n",
        "print(saveFolder_r)\n",
        "print(saveFolder_f)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/real/\n",
            "/content/fake/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GTMbS2qhPE2R",
        "colab_type": "code",
        "outputId": "f7fb012c-aaf2-46f7-e93e-a53c9d14537a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "def _get_epoch_from_checkpoint(x: str) -> int:\n",
        "    return int(x.rstrip('.pth').split('_')[-1])\n",
        "  \n",
        "g_items = gdrive_checkpoint.core.find_items(name=g_pattern)\n",
        "d_items = gdrive_checkpoint.core.find_items(name=d_pattern)\n",
        "sorted_g_filenames = sorted([x.name for x in g_items])\n",
        "sorted_d_filenames = sorted([x.name for x in d_items])\n",
        "\n",
        "# print(sorted_g_filenames)\n",
        "# print(sorted_d_filenames)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-14 15:44:50,584 1145 140017142212480: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_z100_cifar10_epoch%22and+trashed+%3D+false&alt=json\n",
            "2019-04-14 15:44:50,886 1145 140017142212480: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_z100_cifar10_epoch%22and+trashed+%3D+false&alt=json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Zb_NJlUsvoX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# latest_g_epoch = _get_epoch_from_checkpoint(latest_g_filename)\n",
        "# latest_d_epoch = _get_epoch_from_checkpoint(latest_d_filename)\n",
        "\n",
        "# latest_epoch = min(latest_g_epoch, latest_d_epoch)\n",
        "# load_epoch = int(latest_epoch)\n",
        "# # load_epoch = int(latest_epoch) - 1\n",
        "# logger.info(\"Loading from epoch {}\".format(load_epoch))\n",
        "# g_filename = checkpoint_pattern.format(g_pattern, load_epoch)\n",
        "# d_filename = checkpoint_pattern.format(d_pattern, load_epoch)\n",
        "\n",
        "# net_g_fullfilename = sorted_g_filenames[sorted_g_filenames.index(g_filename) - 1]\n",
        "# net_d_fullfilename = sorted_d_filenames[sorted_d_filenames.index(d_filename) - 1]\n",
        "# print(net_g_fullfilename)\n",
        "# print(net_d_fullfilename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vaX2RRDasY0Z",
        "colab_type": "code",
        "outputId": "6d016823-1e4f-447e-e717-75f3cc5efbec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "attempt_reload = True\n",
        "# attempt_reload = False\n",
        "max_load = 0\n",
        "# max_load = None\n",
        "\n",
        "def _download_file_locally(filestr: str) -> None:\n",
        "    remote_files = gdrive_checkpoint.core.find_items(name=filestr)\n",
        "    assert 1 == len(remote_files), str(remote_files)\n",
        "    remote_file = remote_files[0]\n",
        "    gdrive_checkpoint.core.download_file_to_folder(remote_file, filestr)\n",
        "\n",
        "\n",
        "if attempt_reload and len(sorted_g_filenames) > 2 and len(sorted_g_filenames) > 2:\n",
        "    latest_g_filename = max(sorted_g_filenames)\n",
        "    latest_d_filename = max(sorted_d_filenames)\n",
        "\n",
        "    latest_g_epoch = _get_epoch_from_checkpoint(latest_g_filename)\n",
        "    latest_d_epoch = _get_epoch_from_checkpoint(latest_d_filename)\n",
        "\n",
        "    latest_epoch = min(latest_g_epoch, latest_d_epoch)\n",
        "    # load_epoch = int(latest_epoch)\n",
        "    # load_epoch = int(latest_epoch) - 1\n",
        "    logger.info(\"Loading from epoch {}\".format(latest_epoch))\n",
        "    g_filename = checkpoint_pattern.format(g_pattern, latest_epoch)\n",
        "    d_filename = checkpoint_pattern.format(d_pattern, latest_epoch)\n",
        "\n",
        "    last_g_fullfilename = sorted_g_filenames[sorted_g_filenames.index(g_filename) - 1]\n",
        "    last_d_fullfilename = sorted_d_filenames[sorted_d_filenames.index(d_filename) - 1]\n",
        "\n",
        "    d_epoch = _get_epoch_from_checkpoint(last_d_fullfilename)\n",
        "    g_epoch = _get_epoch_from_checkpoint(last_g_fullfilename)\n",
        "    load_epoch = min(d_epoch, g_epoch, max_load)\n",
        "\n",
        "    net_d_fullfilename = checkpoint_pattern.format(d_pattern, load_epoch)\n",
        "    net_g_fullfilename = checkpoint_pattern.format(g_pattern, load_epoch)\n",
        "                \n",
        "    _download_file_locally(net_d_fullfilename)\n",
        "    _download_file_locally(net_g_fullfilename)\n",
        "    # net_g_fullfilename = sorted_g_filenames[sorted_g_filenames.index(g_filename) - 1]\n",
        "    # net_d_fullfilename = sorted_d_filenames[sorted_d_filenames.index(d_filename) - 1]\n",
        "\n",
        "else:\n",
        "    net_g_fullfilename = ''\n",
        "    net_d_fullfilename = ''\n",
        "\n",
        "    min_iter = 0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-14 15:46:19,550 1145 140017142212480: Loading from epoch 4\n",
            "2019-04-14 15:46:19,565 1145 140017142212480: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_z100_cifar10_epoch_0000.pth%22and+trashed+%3D+false&alt=json\n",
            "2019-04-14 15:46:19,902 1145 140017142212480: URL being requested: GET https://www.googleapis.com/drive/v3/files/1OSktGCVyGtLoVdoZ8RmC5E-MXW2mi5yn?alt=media\n",
            "2019-04-14 15:46:21,651 1145 140017142212480: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_z100_cifar10_epoch_0000.pth%22and+trashed+%3D+false&alt=json\n",
            "2019-04-14 15:46:21,952 1145 140017142212480: URL being requested: GET https://www.googleapis.com/drive/v3/files/1IwzIbEk6QMQt4XNkaM1xvlsAXJvMfR20?alt=media\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7berrnr383OD",
        "colab_type": "code",
        "outputId": "0d16e620-0a4d-499a-b011-8d57aeb851cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "cell_type": "code",
      "source": [
        "logger.info(net_g_fullfilename)\n",
        "logger.info(net_d_fullfilename)\n",
        "!ls"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-14 15:47:19,878 1145 140017142212480: netG_z100_cifar10_epoch_0000.pth\n",
            "2019-04-14 15:47:19,883 1145 140017142212480: netD_z100_cifar10_epoch_0000.pth\n",
            "adc.json\t\t\t  netD_z100_cifar10_epoch_0002.pth\n",
            "examples\t\t\t  netD_z100_cifar10_epoch_0003.pth\n",
            "fake\t\t\t\t  netD_z100_cifar10_epoch_0004.pth\n",
            "fake_samples_epoch_000.png\t  netG_z100_cifar10_epoch_0000.pth\n",
            "fake_samples_epoch_001.png\t  netG_z100_cifar10_epoch_0001.pth\n",
            "fake_samples_epoch_002.png\t  netG_z100_cifar10_epoch_0002.pth\n",
            "fake_samples_epoch_003.png\t  netG_z100_cifar10_epoch_0003.pth\n",
            "fake_samples_epoch_004.png\t  netG_z100_cifar10_epoch_0004.pth\n",
            "fake_samples_epoch_005.png\t  real\n",
            "gan_metrics\t\t\t  real_samples.png\n",
            "gdrive_checkpoint\t\t  sample_data\n",
            "netD_z100_cifar10_epoch_0000.pth  score_tr_ep.npy\n",
            "netD_z100_cifar10_epoch_0001.pth  score_tr.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hrcWGX_othO9",
        "colab_type": "code",
        "outputId": "d3891ec1-5eeb-485d-a8a1-710862abad0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        }
      },
      "cell_type": "code",
      "source": [
        "# net_g_filename = \"_{}\".format()\n",
        "# net_d_path = os.path.join(checkpoint_dir, identifier, dataset_name, \"d\")\n",
        "\n",
        "# net_g_fullfilename = os.path.join(net_g_path, net_g_filename)\n",
        "# net_d_fullfilename =\n",
        "\n",
        "# net_g_pattern = '{epoch}'.format({'epoch': 1})\n",
        "# net_d_pattern = ''\n",
        "  \n",
        "if net_g_fullfilename != '':\n",
        "    netG.load_state_dict(torch.load(net_g_fullfilename))\n",
        "\n",
        "if net_d_fullfilename != '':\n",
        "    netD.load_state_dict(torch.load(net_d_fullfilename))\n",
        "\n",
        "logger.info(netD)\n",
        "logger.info(netG)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    os.makedirs(out_folder, exist_ok=True)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    real_label = 1\n",
        "    fake_label = 0\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=num_workers)\n",
        "\n",
        "    # set up optimizer\n",
        "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "\n",
        "    dataloader_size = len(dataloader)\n",
        "    \n",
        "    logger.info(\"Running iterations {} to {}\".format(min_iter, max_iter))\n",
        "    \n",
        "    for epoch in range(min_iter, max_iter):\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "            # train with real\n",
        "            netD.zero_grad()\n",
        "            real_cpu = data[0].to(device)\n",
        "            batch_size = real_cpu.size(0)\n",
        "            label = torch.full((batch_size,), real_label, device=device)\n",
        "\n",
        "            output = netD(real_cpu)\n",
        "            errD_real = criterion(output, label)\n",
        "            errD_real.backward()\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "            # train with fake\n",
        "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "            fake = netG(noise)\n",
        "            label.fill_(fake_label)\n",
        "            output = netD(fake.detach())\n",
        "            errD_fake = criterion(output, label)\n",
        "            errD_fake.backward()\n",
        "            D_G_z1 = output.mean().item()\n",
        "            errD = errD_real + errD_fake\n",
        "            optimizerD.step()\n",
        "\n",
        "            # (2) Update G network: maximize log(D(G(z)))\n",
        "            netG.zero_grad()\n",
        "            label.fill_(real_label)  # fake labels are real for generator cost\n",
        "            output = netD(fake)\n",
        "            errG = criterion(output, label)\n",
        "            errG.backward()\n",
        "            D_G_z2 = output.mean().item()\n",
        "            optimizerG.step()\n",
        "\n",
        "            if i % print_every_iteration == 0:\n",
        "                loss_d = errD.item()\n",
        "                loss_g = errG.item()\n",
        "\n",
        "                logger.info('[%d/%d] [%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "                      % (epoch, max_iter, i, dataloader_size, loss_d, loss_g, D_x, D_G_z1, D_G_z2))\n",
        "            if i % save_every_iteration == 0:\n",
        "                real_filename = '%s/real_samples.png' % out_folder\n",
        "                fake_filename = '%s/fake_samples_epoch_%03d.png' % (out_folder, epoch)\n",
        "\n",
        "                fake = netG(fixed_noise)\n",
        "                \n",
        "                vutils.save_image(real_cpu, real_filename, normalize=True)\n",
        "                vutils.save_image(fake.detach(), fake_filename, normalize=True)\n",
        "                \n",
        "        g_filename = os.path.join(checkpoint_dir, checkpoint_pattern.format(g_pattern, epoch))\n",
        "        d_filename = os.path.join(checkpoint_dir, checkpoint_pattern.format(d_pattern, epoch))\n",
        "\n",
        "        torch.save(netG.state_dict(), g_filename)\n",
        "        torch.save(netD.state_dict(), d_filename)\n",
        "\n",
        "        if 0 == epoch % checkpoint_every_epoch:\n",
        "            logger.info('Checkpointing epoch {}'.format(epoch))\n",
        "\n",
        "            # Delete any existing files with this name, to avoid ending up with multiple files\n",
        "            \n",
        "            # _delete_all_remote_files(g_filename)\n",
        "            # _delete_all_remote_files(d_filename)\n",
        "            \n",
        "            gdrive_checkpoint.core.upload_file_to_folder(g_filename, folder=parent_fid)\n",
        "            gdrive_checkpoint.core.upload_file_to_folder(d_filename, folder=parent_fid) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-14 15:47:32,841 1145 140017142212480: Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n",
            "2019-04-14 15:47:32,843 1145 140017142212480: Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n",
            "2019-04-14 15:47:32,847 1145 140017142212480: Running iterations 0 to 5\n",
            "2019-04-14 15:47:33,914 1145 140017142212480: [0/5] [0/782] Loss_D: 0.3686 Loss_G: 5.7596 D(x): 0.8576 D(G(z)): 0.1406 / 0.0109\n",
            "2019-04-14 15:47:53,254 1145 140017142212480: [0/5] [100/782] Loss_D: 1.2034 Loss_G: 2.2057 D(x): 0.4526 D(G(z)): 0.0737 / 0.1913\n",
            "2019-04-14 15:48:12,297 1145 140017142212480: [0/5] [200/782] Loss_D: 0.6023 Loss_G: 4.0666 D(x): 0.8601 D(G(z)): 0.3129 / 0.0309\n",
            "2019-04-14 15:48:31,546 1145 140017142212480: [0/5] [300/782] Loss_D: 0.9457 Loss_G: 5.4728 D(x): 0.4827 D(G(z)): 0.0223 / 0.0183\n",
            "2019-04-14 15:48:50,574 1145 140017142212480: [0/5] [400/782] Loss_D: 1.2948 Loss_G: 2.8899 D(x): 0.3878 D(G(z)): 0.0142 / 0.0866\n",
            "2019-04-14 15:49:09,808 1145 140017142212480: [0/5] [500/782] Loss_D: 0.7496 Loss_G: 2.3912 D(x): 0.5686 D(G(z)): 0.0436 / 0.1383\n",
            "2019-04-14 15:49:28,834 1145 140017142212480: [0/5] [600/782] Loss_D: 1.8823 Loss_G: 9.1168 D(x): 0.9340 D(G(z)): 0.7778 / 0.0003\n",
            "2019-04-14 15:49:48,062 1145 140017142212480: [0/5] [700/782] Loss_D: 0.5996 Loss_G: 2.8513 D(x): 0.7482 D(G(z)): 0.2025 / 0.0785\n",
            "2019-04-14 15:50:03,857 1145 140017142212480: Checkpointing epoch 0\n",
            "2019-04-14 15:50:03,867 1145 140017142212480: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n",
            "2019-04-14 15:50:05,608 1145 140017142212480: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n",
            "2019-04-14 15:50:07,705 1145 140017142212480: [1/5] [0/782] Loss_D: 0.5826 Loss_G: 3.0081 D(x): 0.7790 D(G(z)): 0.2195 / 0.0639\n",
            "2019-04-14 15:50:26,924 1145 140017142212480: [1/5] [100/782] Loss_D: 0.5174 Loss_G: 2.9034 D(x): 0.7158 D(G(z)): 0.1067 / 0.0920\n",
            "2019-04-14 15:50:45,935 1145 140017142212480: [1/5] [200/782] Loss_D: 0.6875 Loss_G: 3.1343 D(x): 0.6346 D(G(z)): 0.0940 / 0.0880\n",
            "2019-04-14 15:51:05,194 1145 140017142212480: [1/5] [300/782] Loss_D: 0.6884 Loss_G: 2.3542 D(x): 0.6598 D(G(z)): 0.1608 / 0.1205\n",
            "2019-04-14 15:51:24,216 1145 140017142212480: [1/5] [400/782] Loss_D: 0.4241 Loss_G: 4.1578 D(x): 0.8711 D(G(z)): 0.2162 / 0.0228\n",
            "2019-04-14 15:51:43,450 1145 140017142212480: [1/5] [500/782] Loss_D: 0.5724 Loss_G: 4.1612 D(x): 0.8386 D(G(z)): 0.2861 / 0.0223\n",
            "2019-04-14 15:52:02,469 1145 140017142212480: [1/5] [600/782] Loss_D: 0.5247 Loss_G: 2.3048 D(x): 0.7290 D(G(z)): 0.1529 / 0.1252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z5r56O86fUkx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g_filename = os.path.join(checkpoint_dir, checkpoint_pattern.format(g_pattern, epoch))\n",
        "print(g_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NWpPT4drX5yQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "# [emd-mmd-knn(knn,real,fake,precision,recall)]*4 - IS - mode_score - FID\n",
        "score_tr = np.zeros((max_iter, 4*7+3))\n",
        "\n",
        "for epoch in range(0, max_iter):\n",
        "    g_filename = checkpoint_pattern.format(g_pattern, epoch)\n",
        "\n",
        "    logger.info(g_filename)\n",
        "    #     netG.load_state_dict(torch.load(net_g_fullfilename))\n",
        "#     s = metric.compute_score_raw(dataset_name, \n",
        "#                                  image_size, \n",
        "#                                  dataroot, \n",
        "#                                  sample_size, \n",
        "#                                  batch_size, \n",
        "#                                  saveFolder_r=saveFolder_r, \n",
        "#                                  saveFolder_f=saveFolder_f, \n",
        "#                                  netG=netG, \n",
        "#                                  nz=nz, \n",
        "#                                  conv_model='inception_v3', \n",
        "#                                  workers=num_workers)\n",
        "#     score_tr[epoch] = s\n",
        "\n",
        "      \n",
        "# save final metric scores of all epoches\n",
        "np.save('%s/score_tr_ep.npy' % out_folder, score_tr)\n",
        "print('##### training completed :) #####')\n",
        "print('### metric scores output is scored at %s/score_tr_ep.npy ###' % out_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJ6rWgMge4-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !ls MNIST_DCGAN_results/Fixed_results/\n",
        "# !ls MNIST_DCGAN_results/Random_results/\n",
        "# !ls\n",
        "# os.path.split(g_filename)[1]\n",
        "!ls -ltra\n",
        "# net_g_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiiDugNNfjqg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# filename = 'fake_samples_epoch_001.png'\n",
        "filename = 'fake_samples_epoch_016.png'\n",
        "img = mpimg.imread(filename)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PLrWLJsrAIwz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/xuqiantong/GAN-Metrics.git gan_metric\n",
        "# # !ls\n",
        "# import gan_metric.metric as metric\n",
        "# # import metric\n",
        "\n",
        "print(saveFolder_r)\n",
        "\n",
        "print(saveFolder_f)\n",
        "!ls real\n",
        "!ls fake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PKIyTqHoUTDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# https://github.com/kylematoba/GAN-Metrics/blob/master/metric.py#L399\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pfH39rFUKgJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wnJqiE4pTkvk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "saveFolder_r = os.path.join(out_folder, 'real')\n",
        "print(saveFolder_r)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}