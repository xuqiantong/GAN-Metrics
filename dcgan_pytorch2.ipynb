{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan_pytorch2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylematoba/GAN-Metrics/blob/master/dcgan_pytorch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mTrMcMltRSQH",
        "colab_type": "code",
        "outputId": "39c60ff3-d053-4d15-fbd3-8c88651d4643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade setuptools"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hvH7qnpmTI5A",
        "colab_type": "code",
        "outputId": "6d227156-bcbe-4545-ec24-411b40f3112b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# !rm -rf examples\n",
        "!git clone https://github.com/kylematoba/examples.git\n",
        "# !git -C examples log -n 2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'examples' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hfsI5MSHQSJ3",
        "colab_type": "code",
        "outputId": "116a0f8e-3fb7-46da-ae14-26f4e0745683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# !rm -rf gan_metrics\n",
        "!git clone https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/GAN-Metrics.git gan_metrics"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gan_metrics' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gT5mwRBMOrt-",
        "colab_type": "code",
        "outputId": "9ab2cb7e-726b-474a-ad41-bd963eb8e1d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install pot"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pot in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pot) (1.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pot) (1.16.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from pot) (0.29.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pot) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->pot) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->pot) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tdnOpHY1kl7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import sys\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import gan_metrics.metric as metric\n",
        "\n",
        "FORMAT = \"%(asctime)s %(process)s %(thread)s: %(message)s\"\n",
        "logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "dict_environ = dict(os.environ)\n",
        "# logger.info(pprint.pformat(dict_environ, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p71Nrh9AxBkB",
        "colab_type": "code",
        "outputId": "45420dd2-d271-4719-b3b1-10ab4a5cfaf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --force-reinstall git+https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/matobapython.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://kylematoba:****@github.com/kylematoba/matobapython.git\n",
            "  Cloning https://kylematoba:****@github.com/kylematoba/matobapython.git to /tmp/pip-req-build-250zqs1k\n",
            "Building wheels for collected packages: matobapython\n",
            "  Building wheel for matobapython (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-fr80ztey/wheels/e1/2d/7a/3c81733c70f1f3d702f15d4d9f352f995deacb0ee96b476c47\n",
            "Successfully built matobapython\n",
            "Installing collected packages: matobapython\n",
            "  Found existing installation: matobapython 0.0.1\n",
            "    Uninstalling matobapython-0.0.1:\n",
            "      Successfully uninstalled matobapython-0.0.1\n",
            "Successfully installed matobapython-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1w_lu9eoxF9d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pythonutils.gdrive as gdrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "irLFxc4dkpKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataroot = 'examples'\n",
        "# prng_seed = None\n",
        "seed_char = 'a'\n",
        "prng_seed = ord(seed_char)\n",
        "\n",
        "# prng_seed = 8\n",
        "# prng_seed = 1\n",
        "# prng_seed = 10\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "is_cuda = True\n",
        "lr = 0.0002\n",
        "beta1 = .5\n",
        "\n",
        "# max_iter = 25\n",
        "max_iter = 25\n",
        "# max_iter = 3\n",
        "# max_iter = 10\n",
        "num_workers = 2\n",
        "ngpu = 1\n",
        "\n",
        "# nz = 1\n",
        "# nz = 2\n",
        "# nz = 5\n",
        "# nz = 10\n",
        "# nz = 50\n",
        "# nz = 95\n",
        "nz = 100\n",
        "# nz = 200\n",
        "# nz = 500\n",
        "# nz = 1000\n",
        "\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "if prng_seed is None:\n",
        "    prng_seed = random.randint(1, 10000)\n",
        "\n",
        "# assert prng_seed < 1000, \"Not supporting seeds with more than 5 digits\"\n",
        "# identifier = 'ident'\n",
        "# identifier_base = 'z{:05d}seed{:05d}'.format(nz, prng_seed)\n",
        "zstr = 'z{:05d}'.format(nz)\n",
        "seedstr = 'seed_{}'.format(seed_char)\n",
        "\n",
        "identifier = zstr\n",
        "out_folder_local = '/content'\n",
        "\n",
        "# dataset_name = 'lsun'\n",
        "# dataset_name = 'mnist'\n",
        "dataset_name = 'cifar10'\n",
        "\n",
        "print_every_iteration = 200\n",
        "save_every_iteration = 200\n",
        "checkpoint_every_epoch = 2\n",
        "\n",
        "# logger.info(\"Identifier: {}\".format(identifier))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cxciv29hTna5",
        "colab_type": "code",
        "outputId": "c4b8d5cf-3967-4615-b1d4-cfa65e93aa5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "logger.info(\"Random Seed: {}\".format(prng_seed))\n",
        "random.seed(prng_seed)\n",
        "torch.manual_seed(prng_seed)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if torch.cuda.is_available() and not is_cuda:\n",
        "    logger.info(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "if dataset_name in ['imagenet', 'folder', 'lfw']:\n",
        "    dataset = dset.ImageFolder(root=dataroot,\n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.Resize(image_size),\n",
        "                                   transforms.CenterCrop(image_size),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                               ]))\n",
        "    nc=3\n",
        "elif dataset_name == 'lsun':\n",
        "    dataset = dset.LSUN(root=dataroot, classes=['bedroom_train'],\n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.Resize(image_size),\n",
        "                            transforms.CenterCrop(image_size),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                        ]))\n",
        "    nc=3\n",
        "elif dataset_name == 'cifar10':\n",
        "    dataset = dset.CIFAR10(root=dataroot, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "    nc=3\n",
        "elif dataset_name == 'mnist':\n",
        "        dataset = dset.MNIST(root=dataroot, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5,), (0.5,)),\n",
        "                           ]))\n",
        "        nc=1\n",
        "elif dataset_name == 'fake':\n",
        "    dataset = dset.FakeData(image_size=(3, image_size, image_size),\n",
        "                            transform=transforms.ToTensor())\n",
        "    nc=3\n",
        "\n",
        "assert dataset\n",
        "device = torch.device(\"cuda:0\" if is_cuda else \"cpu\")\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu: int):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu: int):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "netD.apply(weights_init)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-28 08:28:39,156 1399 139630618269568: Random Seed: 97\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (12): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "5zx1BHJhM6qK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _delete_all_remote_files(del_filename: str, \n",
        "                             parent_fid: str) -> None:\n",
        "    del_files = gdrive.find_items(name=del_filename, \n",
        "                                  parent_fid=parent_fid, \n",
        "                                  skip_trashed=True)\n",
        "    for x in del_files:\n",
        "        logger.info(\"Deleting {}\".format(x))\n",
        "        gdrive.delete_file(x)\n",
        "        \n",
        "        \n",
        "def _create_folder_in_parent(folder_name: str, \n",
        "                             parent_fid: str, \n",
        "                             exist_ok: bool) -> str:\n",
        "    found_folders = gdrive.find_items(folder_name, \n",
        "                                      parent_fid=parent_fid, \n",
        "                                      skip_trashed=True)\n",
        "    num_found = len(found_folders)\n",
        "    assert num_found <= 1, \"Multiple matches, refine query\"\n",
        "\n",
        "    if 1 == num_found:\n",
        "        found_folder = found_folders[0] \n",
        "        folder_fid = found_folder[1]\n",
        "        logger.info(\"Found it, {}\".format(folder_fid))\n",
        "        assert exist_ok, \"Not expecting to find it\"\n",
        "    else:\n",
        "        created_folder = gdrive.create_folder(folder_name=folder_name, \n",
        "                                              parent_fid=parent_fid)\n",
        "        folder_fid = created_folder[1]\n",
        "        logger.info(\"Not found, created with fid = {}\".format(folder_fid))\n",
        "    return folder_fid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6r8HTg7ULvRj",
        "colab_type": "code",
        "outputId": "d0524946-8ee5-4225-e881-7ad15694959e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "cell_type": "code",
      "source": [
        "# Check whether there are checkpoints in the google drive\n",
        "username = 'robotmatoba'\n",
        "gdrive.authenticate_automatically(username)\n",
        "\n",
        "base_folder_name = 'PytorchCheckpoints'\n",
        "\n",
        "logger.info(\"Setting up base folder '{}'\".format(base_folder_name))\n",
        "base_folder_fid = _create_folder_in_parent(folder_name=base_folder_name, parent_fid=None, exist_ok=True)\n",
        "\n",
        "logger.info(\"Setting up dataset folder '{}' in '{}'\".format(dataset_name, base_folder_name))\n",
        "dataset_folder_fid = _create_folder_in_parent(folder_name=dataset_name, parent_fid=base_folder_fid, exist_ok=True)\n",
        "\n",
        "logger.info(\"Setting up dataset/z/ folder '{}' in '{}'\".format(zstr, dataset_name))\n",
        "datasetz_folder_fid = _create_folder_in_parent(folder_name=zstr, parent_fid=dataset_folder_fid, exist_ok=True)\n",
        "\n",
        "logger.info(\"Setting up dataset/z/seed folder '{}' in '{}'\".format(seedstr, zstr))\n",
        "datasetzseed_folder_fid = _create_folder_in_parent(folder_name=seedstr, parent_fid=datasetz_folder_fid, exist_ok=True)\n",
        "\n",
        "# logger.info(\"Creating experiment subfolder '{}' with parent fid '{}'\".format(dataset_name, base_fid))\n",
        "# dataset_folder = gdrive.create_folder(dataset_name, [base_fid.fid])\n",
        "# # identifier_folder = gdrive.create_folder(dataset_name, [base_fid.fid])\n",
        "# # Folder structure is:\n",
        "# # dataset/nz/\n",
        "\n",
        "parent_fid = datasetzseed_folder_fid"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-28 08:28:44,568 1399 139630618269568: Setting up base folder 'PytorchCheckpoints'\n",
            "2019-04-28 08:28:44,569 1399 139630618269568: Submitting query name contains \"PytorchCheckpoints\" and trashed = false\n",
            "2019-04-28 08:28:44,575 1399 139630618269568: file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "2019-04-28 08:28:44,576 1399 139630618269568: URL being requested: GET https://www.googleapis.com/discovery/v1/apis/drive/v3/rest\n",
            "2019-04-28 08:28:44,593 1399 139630618269568: No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
            "2019-04-28 08:28:44,599 1399 139630618269568: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22PytorchCheckpoints%22+and+trashed+%3D+false&alt=json\n",
            "2019-04-28 08:28:44,773 1399 139630618269568: Found it, 1t4s77dg0G5hm0TlHvnrQkBYpJ64wuHml\n",
            "2019-04-28 08:28:44,774 1399 139630618269568: Setting up dataset folder 'cifar10' in 'PytorchCheckpoints'\n",
            "2019-04-28 08:28:44,774 1399 139630618269568: Submitting query name contains \"cifar10\" and \"1t4s77dg0G5hm0TlHvnrQkBYpJ64wuHml\" in parents and trashed = false\n",
            "2019-04-28 08:28:44,784 1399 139630618269568: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22cifar10%22+and+%221t4s77dg0G5hm0TlHvnrQkBYpJ64wuHml%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-04-28 08:28:44,942 1399 139630618269568: Found it, 1_3XHVYiPP8uoSvr-J5SYE97or8rgCecF\n",
            "2019-04-28 08:28:44,942 1399 139630618269568: Setting up dataset/z/ folder 'z00100' in 'cifar10'\n",
            "2019-04-28 08:28:44,943 1399 139630618269568: Submitting query name contains \"z00100\" and \"1_3XHVYiPP8uoSvr-J5SYE97or8rgCecF\" in parents and trashed = false\n",
            "2019-04-28 08:28:44,949 1399 139630618269568: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22z00100%22+and+%221_3XHVYiPP8uoSvr-J5SYE97or8rgCecF%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-04-28 08:28:45,069 1399 139630618269568: Found it, 1dGq4sUS_fkpGlp6W1LxrT7k_fFPTtZj8\n",
            "2019-04-28 08:28:45,070 1399 139630618269568: Setting up dataset/z/seed folder 'seed_a' in 'z00100'\n",
            "2019-04-28 08:28:45,071 1399 139630618269568: Submitting query name contains \"seed_a\" and \"1dGq4sUS_fkpGlp6W1LxrT7k_fFPTtZj8\" in parents and trashed = false\n",
            "2019-04-28 08:28:45,080 1399 139630618269568: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22seed_a%22+and+%221dGq4sUS_fkpGlp6W1LxrT7k_fFPTtZj8%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-04-28 08:28:45,213 1399 139630618269568: Found it, 1axzhPe35bUlsACUDjn7y_OILkUuWWr4w\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0JKJKMiQz01p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net_g_pattern = 'netG_epoch'\n",
        "net_d_pattern = 'netD_epoch'\n",
        "\n",
        "checkpoint_pattern = '{}{:04d}.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onmQ-Nd3bXFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_folder_r = os.path.join(out_folder_local, 'real/')\n",
        "save_folder_f = os.path.join(out_folder_local, 'fake/')\n",
        "\n",
        "os.makedirs(save_folder_r, exist_ok=True)\n",
        "os.makedirs(save_folder_f, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GTMbS2qhPE2R",
        "colab_type": "code",
        "outputId": "481c1175-12cf-426b-e20b-8ab967e0d1ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "net_g_items = gdrive.find_items(name=net_g_pattern, parent_fid=parent_fid, skip_trashed=True)\n",
        "net_d_items = gdrive.find_items(name=net_d_pattern, parent_fid=parent_fid, skip_trashed=True)\n",
        "sorted_net_g_filenames = sorted([x.name for x in net_g_items])\n",
        "sorted_net_d_filenames = sorted([x.name for x in net_d_items])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-28 08:28:45,239 1399 139630618269568: Submitting query name contains \"netG_epoch\" and \"1axzhPe35bUlsACUDjn7y_OILkUuWWr4w\" in parents and trashed = false\n",
            "2019-04-28 08:28:45,246 1399 139630618269568: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_epoch%22+and+%221axzhPe35bUlsACUDjn7y_OILkUuWWr4w%22+in+parents+and+trashed+%3D+false&alt=json\n",
            "2019-04-28 08:28:45,375 1399 139630618269568: Submitting query name contains \"netD_epoch\" and \"1axzhPe35bUlsACUDjn7y_OILkUuWWr4w\" in parents and trashed = false\n",
            "2019-04-28 08:28:45,378 1399 139630618269568: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_epoch%22+and+%221axzhPe35bUlsACUDjn7y_OILkUuWWr4w%22+in+parents+and+trashed+%3D+false&alt=json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vaX2RRDasY0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "attempt_reload = True\n",
        "# attempt_reload = False\n",
        "# max_load = 0\n",
        "max_load = None\n",
        "\n",
        "\n",
        "def _get_epoch_from_checkpoint(x: str) -> int:\n",
        "    return int(x.rstrip('.pth').split('epoch')[-1])\n",
        "  \n",
        "  \n",
        "def _download_file_locally(filestr: str, \n",
        "                           parent_fid: str) -> None:\n",
        "    remote_files = gdrive.find_items(name=filestr, parent=parent_fid, skip_trashed=True)\n",
        "    assert 1 == len(remote_files), str(remote_files)\n",
        "    remote_file = remote_files[0]\n",
        "    gdrive.download_file_to_folder(remote_file, filestr)\n",
        "\n",
        "\n",
        "if attempt_reload and len(sorted_net_g_filenames) > 2 and len(sorted_net_d_filenames) > 2:\n",
        "    latest_net_g_filename = max(sorted_net_g_filenames)\n",
        "    latest_net_d_filename = max(sorted_net_d_filenames)\n",
        "\n",
        "    latest_net_g_epoch = _get_epoch_from_checkpoint(latest_net_g_filename)\n",
        "    latest_net_d_epoch = _get_epoch_from_checkpoint(latest_net_d_filename)\n",
        "\n",
        "    latest_epoch = min(latest_net_g_epoch, latest_net_d_epoch)\n",
        "\n",
        "    net_g_filename = checkpoint_pattern.format(net_g_pattern, latest_epoch)\n",
        "    net_d_filename = checkpoint_pattern.format(net_d_pattern, latest_epoch)\n",
        "\n",
        "    last_net_g_fullfilename = sorted_net_g_filenames[sorted_net_g_filenames.index(net_g_filename) - 1]\n",
        "    last_net_d_fullfilename = sorted_net_d_filenames[sorted_net_d_filenames.index(net_d_filename) - 1]\n",
        "\n",
        "    g_epoch = _get_epoch_from_checkpoint(last_net_g_fullfilename)\n",
        "    d_epoch = _get_epoch_from_checkpoint(last_net_d_fullfilename)\n",
        "\n",
        "    load_epoch = min(d_epoch, g_epoch)\n",
        "    if max_load is not None:\n",
        "       load_epoch = min(load_epoch, max_load)\n",
        "        \n",
        "    net_g_fullfilename = checkpoint_pattern.format(net_g_pattern, load_epoch)\n",
        "    net_d_fullfilename = checkpoint_pattern.format(net_d_pattern, load_epoch)\n",
        "                \n",
        "    _download_file_locally(net_d_fullfilename, parent_fid)\n",
        "    _download_file_locally(net_g_fullfilename, parent_fid)\n",
        "    min_iter = load_epoch\n",
        "    logger.info(\"Loading from epoch {:04d}\".format(load_epoch))\n",
        "\n",
        "else:\n",
        "    net_g_fullfilename = ''\n",
        "    net_d_fullfilename = ''\n",
        "\n",
        "    min_iter = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NqZzu7HYCGmg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# logger.info(\"Will reload from {} and {}\".format(net_g_fullfilename, net_d_fullfilename))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hrcWGX_othO9",
        "colab_type": "code",
        "outputId": "8a2f07bf-286e-4209-8a45-0855cb17fc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        }
      },
      "cell_type": "code",
      "source": [
        "if net_g_fullfilename != '':\n",
        "    netG.load_state_dict(torch.load(net_g_fullfilename))\n",
        "\n",
        "if net_d_fullfilename != '':\n",
        "    netD.load_state_dict(torch.load(net_d_fullfilename))\n",
        "\n",
        "logger.info(netD)\n",
        "logger.info(netG)\n",
        "\n",
        "checkpoint_dir = out_folder_local\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    os.makedirs(out_folder_local, exist_ok=True)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    real_label = 1\n",
        "    fake_label = 0\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=num_workers)\n",
        "\n",
        "    # set up optimizer\n",
        "    optimizer_d = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "    optimizer_g = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "    dataloader_size = len(dataloader)\n",
        "    \n",
        "    logger.info(\"Running iterations {} to {}\".format(min_iter, max_iter))\n",
        "    \n",
        "    for epoch in range(min_iter, max_iter):\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "            # train with real\n",
        "            netD.zero_grad()\n",
        "            real_cpu = data[0].to(device)\n",
        "            batch_size = real_cpu.size(0)\n",
        "            label = torch.full((batch_size,), real_label, device=device)\n",
        "\n",
        "            output = netD(real_cpu)\n",
        "            errD_real = criterion(output, label)\n",
        "            errD_real.backward()\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "            # train with fake\n",
        "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "            fake = netG(noise)\n",
        "            label.fill_(fake_label)\n",
        "            output = netD(fake.detach())\n",
        "            errD_fake = criterion(output, label)\n",
        "            errD_fake.backward()\n",
        "            D_G_z1 = output.mean().item()\n",
        "            errD = errD_real + errD_fake\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # (2) Update G network: maximize log(D(G(z)))\n",
        "            netG.zero_grad()\n",
        "            label.fill_(real_label)  # fake labels are real for generator cost\n",
        "            output = netD(fake)\n",
        "            errG = criterion(output, label)\n",
        "            errG.backward()\n",
        "            D_G_z2 = output.mean().item()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            if i % print_every_iteration == 0:\n",
        "                loss_d = errD.item()\n",
        "                loss_g = errG.item()\n",
        "\n",
        "                logger.info('[%d/%d] [%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "                      % (epoch, max_iter, i, dataloader_size, loss_d, loss_g, D_x, D_G_z1, D_G_z2))\n",
        "            if i % save_every_iteration == 0:\n",
        "                real_filename = '%s/real_samples.png' % out_folder_local\n",
        "                fake_filename = '%s/fake_samples_epoch_%03d.png' % (out_folder_local, epoch)\n",
        "\n",
        "                fake = netG(fixed_noise)\n",
        "                \n",
        "                vutils.save_image(real_cpu, real_filename, normalize=True)\n",
        "                vutils.save_image(fake.detach(), fake_filename, normalize=True)\n",
        "                \n",
        "        net_g_filename = checkpoint_pattern.format(net_g_pattern, epoch)\n",
        "        net_d_filename = checkpoint_pattern.format(net_d_pattern, epoch)\n",
        "\n",
        "        net_g_full_filename = os.path.join(checkpoint_dir, net_g_filename)\n",
        "        net_d_full_filename = os.path.join(checkpoint_dir, net_d_filename)\n",
        "\n",
        "        torch.save(netG.state_dict(), net_g_full_filename)\n",
        "        torch.save(netD.state_dict(), net_d_full_filename)\n",
        "\n",
        "        if 0 == epoch % checkpoint_every_epoch:\n",
        "            logger.info('Checkpointing epoch {}'.format(epoch))\n",
        "\n",
        "            # Delete any existing files with this name, to avoid ending up with multiple files\n",
        "            _delete_all_remote_files(net_g_filename, parent_fid=parent_fid)\n",
        "            _delete_all_remote_files(net_d_filename, parent_fid=parent_fid)\n",
        "            \n",
        "            gdrive.upload_file_to_folder(local_file=net_g_filename, parent_fid=parent_fid)\n",
        "            gdrive.upload_file_to_folder(local_file=net_d_filename, parent_fid=parent_fid) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-28 08:28:45,674 1399 139630618269568: Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n",
            "2019-04-28 08:28:45,676 1399 139630618269568: Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n",
            "2019-04-28 08:28:45,679 1399 139630618269568: Running iterations 0 to 25\n",
            "2019-04-28 08:28:46,193 1399 139630618269568: [0/25] [0/782] Loss_D: 1.8206 Loss_G: 4.1236 D(x): 0.3515 D(G(z)): 0.3746 / 0.0206\n",
            "2019-04-28 08:29:00,639 1399 139630618269568: [0/25] [200/782] Loss_D: 0.1359 Loss_G: 6.7717 D(x): 0.9508 D(G(z)): 0.0675 / 0.0017\n",
            "2019-04-28 08:29:15,353 1399 139630618269568: [0/25] [400/782] Loss_D: 0.1882 Loss_G: 5.7750 D(x): 0.8571 D(G(z)): 0.0046 / 0.0075\n",
            "2019-04-28 08:29:30,002 1399 139630618269568: [0/25] [600/782] Loss_D: 0.5939 Loss_G: 3.0635 D(x): 0.7646 D(G(z)): 0.2217 / 0.0610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EW9WIm2JvMy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metrics_pattern = \"metrics_{}\".format(identifier)\n",
        "sample_size = 2000\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# [emd-mmd-knn(knn,real,fake,precision,recall)]*4 - IS - mode_score - FID\n",
        "score_tr = np.zeros((max_iter, 4*7+3))\n",
        "# max_iter = 5\n",
        "for epoch in range(0, max_iter):\n",
        "    metrics_filename = \"{}_{:04d}.npy\".format(metrics_pattern, epoch)\n",
        "    logger.info(metrics_filename)\n",
        "    found_items = gdrive.find_items(name=metrics_filename, parent=None, skip_trashed=True)\n",
        "    \n",
        "    if len(found_items) > 0:\n",
        "        assert len(found_items) <= 1\n",
        "        found_item = found_items[0]\n",
        "        assert found_item.name == metrics_filename\n",
        "        _download_file_locally(metrics_filename, parent_fid)\n",
        "        s = np.load(metrics_filename)\n",
        "    else:\n",
        "        net_g_filename = checkpoint_pattern.format(net_g_pattern, epoch)\n",
        "\n",
        "        logger.info(\"Downloading {}\".format(net_g_filename))\n",
        "        try:\n",
        "            _download_file_locally(net_g_filename, parent_fid)\n",
        "            netG.load_state_dict(torch.load(net_g_filename))\n",
        "            logger.info(\"Computing metrics on {}\".format(net_g_filename))\n",
        "\n",
        "            s = metric.compute_score_raw(dataset_name, \n",
        "                                         image_size, \n",
        "                                         dataroot, \n",
        "                                         sample_size, \n",
        "                                         batch_size, \n",
        "                                         saveFolder_r=save_folder_r, \n",
        "                                         saveFolder_f=save_folder_f, \n",
        "                                         netG=netG, \n",
        "                                         nz=nz, \n",
        "                                         conv_model='inception_v3', \n",
        "                                         workers=num_workers)\n",
        "            np.save(metrics_filename, s)    \n",
        "            _delete_all_remote_files(metrics_filename, parent_fid=parent_fid)\n",
        "            gdrive.upload_file_to_folder(metrics_filename, parent_fid=parent_fid)\n",
        "        except: \n",
        "          s = np.nan\n",
        "    score_tr[epoch, :] = s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j-q7ZeEnBW4t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_rows = np.any(np.isfinite(score_tr), axis=1)\n",
        "plot_score_tr = score_tr[plot_rows, :]\n",
        "plot_axis = np.arange(len(plot_rows))[plot_rows] \n",
        "# print(plot_score_tr)\n",
        "# print(plot_axis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yctPegCoY-2M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metric_names = np.array(['pixl_wasserstein', 'pixl_mmd', 'pixl_acc', 'pixl_acc_t',\n",
        "                         'pixl_acc_f', 'pixl_precision', 'pixl_recall', 'conv_wasserstein',\n",
        "                         'conv_mmd', 'conv_acc', 'conv_acc_t', 'conv_acc_f',\n",
        "                         'conv_precision', 'conv_recall', 'logit_wasserstein', 'logit_mmd',\n",
        "                         'logit_acc', 'logit_acc_t', 'logit_acc_f', 'logit_precision',\n",
        "                         'logit_recall', 'smax_wasserstein', 'smax_mmd', 'smax_acc',\n",
        "                         'smax_acc_t', 'smax_acc_f', 'smax_precision', 'smax_recall',\n",
        "                         'inception_score', 'mode_score', 'fid'], dtype=object)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RilWbGeplNbs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/Lyken17/pytorch-OpCounter.git\n",
        "\n",
        "#   # https://github.community/t5/How-to-use-Git-and-GitHub/Clone-private-repo/td-p/12616\n",
        "# !rm -rf matobapython\n",
        "# !git clone https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/matobapython.git\n",
        "    \n",
        "import thop  \n",
        "# from thop import profile\n",
        "# type(netG)\n",
        "# [16, 1000, 1, 1]\n",
        "flops_g, params_g = thop.profile(netG, input_size=(16, nz, 1, 1))\n",
        "flops_d, params_d = thop.profile(netD, input_size=(16, nc, 64, 64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lL15F5iALRjE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pythonutils.plotting as plotting\n",
        "import pythonutils.send_email as send_email\n",
        "\n",
        "# import inspect\n",
        "# inspect.getsourcelines(send_email)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1mNh6txYD8r-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "num_metrics = len(metric_names)\n",
        "\n",
        "metrics_np = np.full((num_metrics, ), np.nan)\n",
        "for idx in range(num_metrics):\n",
        "    metric_name = metric_names[idx]\n",
        "    metric_value = plot_score_tr[-1, idx]\n",
        "    metrics_np[idx] = metric_value\n",
        "\n",
        "metrics_pd = pd.Series(metrics_np, index=metric_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VM-2QHVbkg9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "attachments = []\n",
        "\n",
        "scale = .5\n",
        "for idx in range(num_metrics):  # idx = 0\n",
        "    metric_name = metric_names[idx]\n",
        "    logger.info(\"Plotting {}\".format(metric_name))\n",
        "    fig = plt.figure(figsize=(12 * scale, 4 * scale))\n",
        "    plt.plot(plot_axis, plot_score_tr[:, idx])\n",
        "    plt.title(metric_name)\n",
        "    ident = \"fig{:05d}\".format(idx)\n",
        "    fig_path = plotting.smart_save_fig(fig, ident=ident)\n",
        "    plt.close(fig)\n",
        "    attachments.append(fig_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2JCm-D8jD0L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lines = []\n",
        "lines += [\"Generator params, flops = {:.0f}, {:.0f}\".format(params_g, flops_g)]\n",
        "lines += [\"Discriminator params, flops = {:.0f}, {:.0f}\".format(flops_d, params_d)]\n",
        "lines += [\"Metrics after {} iterations ({}, seed = {})\".format(max_iter, identifier, prng_seed)]\n",
        "lines += [\"\\n\" + metrics_pd.to_string()]\n",
        "lines += [\"Real, then fake, images follow below\"]\n",
        "\n",
        "message = \"\\n\".join(lines)\n",
        "# logger.info(message)\n",
        "\n",
        "to_addrs = ['kylematoba@gmail.com']\n",
        "subject = 'GAN analysis results ({identifier})'.format(identifier=identifier)\n",
        "# logger.info(subject)\n",
        "\n",
        "attachments = [real_filename, fake_filename] + attachments\n",
        "\n",
        "send_email.send_mail_from_robotmatoba(to_addrs,\n",
        "                                      subject,\n",
        "                                      message, \n",
        "                                      attachments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiiDugNNfjqg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# filename = 'fake_samples_epoch_001.png'\n",
        "# filename = real_filename\n",
        "filename = fake_filename\n",
        "img = matplotlib.image.imread(filename)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}