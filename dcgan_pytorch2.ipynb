{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan_pytorch2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylematoba/GAN-Metrics/blob/master/dcgan_pytorch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hvH7qnpmTI5A",
        "colab_type": "code",
        "outputId": "c3be5d6c-ea5f-45b9-e033-d5c24236ce68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kylematoba/examples.git\n",
        "!git -C examples log -n 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'examples' already exists and is not an empty directory.\n",
            "\u001b[33mcommit 73626a839f0ab7c7458dcde5c1a438bda0757fd9\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m\n",
            "Author: kylematoba <km3227@columbia.edu>\n",
            "Date:   Sat Apr 6 22:27:02 2019 +0100\n",
            "\n",
            "    Created using Colaboratory\n",
            "\n",
            "\u001b[33mcommit 5e91a5d17b2976cf95600cd25f658d469eeab84d\u001b[m\n",
            "Author: kylematoba <km3227@columbia.edu>\n",
            "Date:   Wed Apr 3 16:15:31 2019 +0100\n",
            "\n",
            "    Created using Colaboratory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gwSHE_dQZg2U",
        "colab_type": "code",
        "outputId": "84901e64-296f-4f61-9397-e8236f9f8f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "# https://github.community/t5/How-to-use-Git-and-GitHub/Clone-private-repo/td-p/12616\n",
        "!rm -rf gdrive_checkpoint\n",
        "!git clone https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/gdrive_checkpoint.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gdrive_checkpoint'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 33 (delta 16), reused 28 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hfsI5MSHQSJ3",
        "colab_type": "code",
        "outputId": "10e06400-0541-47d6-cdbb-f3488b48fc41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf gan_metrics\n",
        "!git clone https://kylematoba:!!czsnd889.!!!!@github.com/kylematoba/GAN-Metrics.git gan_metrics"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gan_metrics'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 175 (delta 4), reused 1 (delta 0), pack-reused 161\u001b[K\n",
            "Receiving objects: 100% (175/175), 47.45 MiB | 11.07 MiB/s, done.\n",
            "Resolving deltas: 100% (97/97), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gT5mwRBMOrt-",
        "colab_type": "code",
        "outputId": "b419fa01-4b48-4389-d06f-5aa759459545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install pot"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pot in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pot) (1.2.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from pot) (0.29.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pot) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pot) (1.16.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pot) (2.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->pot) (40.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->pot) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tdnOpHY1kl7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import sys\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import gan_metrics.metric as metric\n",
        "\n",
        "FORMAT = \"%(asctime)s %(process)s %(thread)s: %(message)s\"\n",
        "logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "dict_environ = dict(os.environ)\n",
        "# logger.info(pprint.pformat(dict_environ, indent=4))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "irLFxc4dkpKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "dataroot = 'examples'\n",
        "prng_seed = None\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "is_cuda = True\n",
        "lr = 0.0002\n",
        "beta1 = .5\n",
        "\n",
        "# max_iter = 25\n",
        "# max_iter = 3\n",
        "max_iter = 10\n",
        "num_workers = 2\n",
        "ngpu = 1\n",
        "nz = 100\n",
        "# nz = 50\n",
        "\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "\n",
        "# identifier = 'ident'\n",
        "identifier_base = 'z{:03d}'.format(nz)\n",
        "out_folder = '/content'\n",
        "# dataset_name = 'mnist'\n",
        "dataset_name = 'cifar10'\n",
        "identifier = identifier_base + '_' + dataset_name\n",
        "\n",
        "print_every_iteration = 100\n",
        "save_every_iteration = 200\n",
        "checkpoint_every_epoch = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cxciv29hTna5",
        "colab_type": "code",
        "outputId": "46ab4c3a-0c94-424d-d1ee-91dc5730e92a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "if prng_seed is None:\n",
        "    prng_seed = random.randint(1, 10000)\n",
        "\n",
        "logger.info(\"Random Seed: {}\".format(prng_seed))\n",
        "random.seed(prng_seed)\n",
        "torch.manual_seed(prng_seed)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if torch.cuda.is_available() and not is_cuda:\n",
        "    logger.info(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "if dataset_name in ['imagenet', 'folder', 'lfw']:\n",
        "    # folder dataset\n",
        "    dataset = dset.ImageFolder(root=dataroot,\n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.Resize(image_size),\n",
        "                                   transforms.CenterCrop(image_size),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                               ]))\n",
        "    nc=3\n",
        "elif dataset_name == 'lsun':\n",
        "    dataset = dset.LSUN(root=dataroot, classes=['bedroom_train'],\n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.Resize(image_size),\n",
        "                            transforms.CenterCrop(image_size),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                        ]))\n",
        "    nc=3\n",
        "elif dataset_name == 'cifar10':\n",
        "    dataset = dset.CIFAR10(root=dataroot, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "    nc=3\n",
        "\n",
        "elif dataset_name == 'mnist':\n",
        "        dataset = dset.MNIST(root=dataroot, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5,), (0.5,)),\n",
        "                           ]))\n",
        "        nc=1\n",
        "\n",
        "elif dataset_name == 'fake':\n",
        "    dataset = dset.FakeData(image_size=(3, image_size, image_size),\n",
        "                            transform=transforms.ToTensor())\n",
        "    nc=3\n",
        "\n",
        "assert dataset\n",
        "device = torch.device(\"cuda:0\" if is_cuda else \"cpu\")\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu: int):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu: int):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "netD.apply(weights_init)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-15 10:26:33,174 496 140124913129344: Random Seed: 1630\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (12): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "5zx1BHJhM6qK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gdrive_checkpoint.core\n",
        "import gdrive_checkpoint.utils\n",
        "\n",
        "gdrive_checkpoint.core.authenticate_automatically()\n",
        "\n",
        "\n",
        "def _delete_all_remote_files(del_filename: str) -> None:\n",
        "    del_files = gdrive_checkpoint.core.find_items(name=del_filename)\n",
        "    for x in del_files:\n",
        "        logger.info(\"Deleting {}\".format(x))\n",
        "        gdrive_checkpoint.core.delete_file(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6r8HTg7ULvRj",
        "colab_type": "code",
        "outputId": "6280e092-66e6-4e1d-ec01-46dae8f4ca0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "cell_type": "code",
      "source": [
        "# Check whether there are checkpoints in the google drive\n",
        "gdrive_checkpoint.core.authenticate_automatically()\n",
        "checkpoint_dir = out_folder\n",
        "\n",
        "folder_name = 'PytorchCheckpoints'\n",
        "found_folders = gdrive_checkpoint.core.find_items(folder_name)\n",
        "print(found_folders)\n",
        "if len(found_folders) > 0:\n",
        "    parent_fid = found_folders[0]\n",
        "else:\n",
        "    parent_fid = gdrive_checkpoint.core.create_folder(folder_name)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-15 10:26:37,171 496 140124913129344: file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "2019-04-15 10:26:37,173 496 140124913129344: URL being requested: GET https://www.googleapis.com/discovery/v1/apis/drive/v3/rest\n",
            "2019-04-15 10:26:37,199 496 140124913129344: No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
            "2019-04-15 10:26:37,208 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22PytorchCheckpoints%22and+trashed+%3D+false&alt=json\n",
            "[GDriveItem(name='PytorchCheckpoints', fid='1C7cZgTWt9R_Hmy0bkHPrCRKUxr58qrLX')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0JKJKMiQz01p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g_pattern = 'netG_{}_epoch'.format(identifier)\n",
        "d_pattern = 'netD_{}_epoch'.format(identifier)\n",
        "checkpoint_pattern = '{}_{:04d}.pth'\n",
        "\n",
        "# net_g_path = os.path.join(checkpoint_dir, identifier, dataset_name, \"g\")\n",
        "# net_d_path = os.path.join(checkpoint_dir, identifier, dataset_name, \"d\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onmQ-Nd3bXFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "saveFolder_r = os.path.join(out_folder, 'real/')\n",
        "saveFolder_f = os.path.join(out_folder, 'fake/')\n",
        "\n",
        "os.makedirs(saveFolder_r, exist_ok=True)\n",
        "os.makedirs(saveFolder_f, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GTMbS2qhPE2R",
        "colab_type": "code",
        "outputId": "7c4d7d1f-c0e9-4635-b716-dcfa3358568f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "def _get_epoch_from_checkpoint(x: str) -> int:\n",
        "    return int(x.rstrip('.pth').split('_')[-1])\n",
        "  \n",
        "g_items = gdrive_checkpoint.core.find_items(name=g_pattern)\n",
        "d_items = gdrive_checkpoint.core.find_items(name=d_pattern)\n",
        "sorted_g_filenames = sorted([x.name for x in g_items])\n",
        "sorted_d_filenames = sorted([x.name for x in d_items])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-15 10:26:37,780 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_z100_cifar10_epoch%22and+trashed+%3D+false&alt=json\n",
            "2019-04-15 10:26:38,187 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_z100_cifar10_epoch%22and+trashed+%3D+false&alt=json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vaX2RRDasY0Z",
        "colab_type": "code",
        "outputId": "06275282-7aab-499f-f22d-26e2210cd441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "attempt_reload = True\n",
        "# attempt_reload = False\n",
        "# max_load = 0\n",
        "max_load = None\n",
        "\n",
        "def _download_file_locally(filestr: str) -> None:\n",
        "    remote_files = gdrive_checkpoint.core.find_items(name=filestr)\n",
        "    assert 1 == len(remote_files), str(remote_files)\n",
        "    remote_file = remote_files[0]\n",
        "    gdrive_checkpoint.core.download_file_to_folder(remote_file, filestr)\n",
        "\n",
        "\n",
        "if attempt_reload and len(sorted_g_filenames) > 2 and len(sorted_g_filenames) > 2:\n",
        "    latest_g_filename = max(sorted_g_filenames)\n",
        "    latest_d_filename = max(sorted_d_filenames)\n",
        "\n",
        "    latest_g_epoch = _get_epoch_from_checkpoint(latest_g_filename)\n",
        "    latest_d_epoch = _get_epoch_from_checkpoint(latest_d_filename)\n",
        "\n",
        "    latest_epoch = min(latest_g_epoch, latest_d_epoch)\n",
        "\n",
        "    g_filename = checkpoint_pattern.format(g_pattern, latest_epoch)\n",
        "    d_filename = checkpoint_pattern.format(d_pattern, latest_epoch)\n",
        "\n",
        "    last_g_fullfilename = sorted_g_filenames[sorted_g_filenames.index(g_filename) - 1]\n",
        "    last_d_fullfilename = sorted_d_filenames[sorted_d_filenames.index(d_filename) - 1]\n",
        "\n",
        "    d_epoch = _get_epoch_from_checkpoint(last_d_fullfilename)\n",
        "    g_epoch = _get_epoch_from_checkpoint(last_g_fullfilename)\n",
        "    load_epoch = min(d_epoch, g_epoch)\n",
        "    if max_load is not None:\n",
        "       load_epoch = min(load_epoch, max_load)\n",
        "\n",
        "    net_d_fullfilename = checkpoint_pattern.format(d_pattern, load_epoch)\n",
        "    net_g_fullfilename = checkpoint_pattern.format(g_pattern, load_epoch)\n",
        "                \n",
        "    _download_file_locally(net_d_fullfilename)\n",
        "    _download_file_locally(net_g_fullfilename)\n",
        "    min_iter = load_epoch\n",
        "    logger.info(\"Loading from epoch {}\".format(load_epoch))\n",
        "\n",
        "else:\n",
        "    net_g_fullfilename = ''\n",
        "    net_d_fullfilename = ''\n",
        "\n",
        "    min_iter = 0"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-15 10:37:57,417 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_z100_cifar10_epoch_0001.pth%22and+trashed+%3D+false&alt=json\n",
            "2019-04-15 10:37:57,776 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files/1_O3Rx_JVWKtzZJYuJHSpYYA2sTryhQtg?alt=media\n",
            "2019-04-15 10:37:58,680 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_z100_cifar10_epoch_0001.pth%22and+trashed+%3D+false&alt=json\n",
            "2019-04-15 10:37:59,019 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files/13qo4lIu5Ccq_2GO7n2nS4bznpTH40aUU?alt=media\n",
            "2019-04-15 10:37:59,639 496 140124913129344: Loading from epoch 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rSqA7ppckFkU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f7a9a2d-684d-4a3a-ce11-24cdd65956b5"
      },
      "cell_type": "code",
      "source": [
        "sorted_g_filenames[sorted_g_filenames.index(g_filename) - 1]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'netG_z100_cifar10_epoch_0001.pth'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "hrcWGX_othO9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2086
        },
        "outputId": "cd8ce911-cda3-4f75-f7bb-fdf89ddefe5a"
      },
      "cell_type": "code",
      "source": [
        "if net_g_fullfilename != '':\n",
        "    netG.load_state_dict(torch.load(net_g_fullfilename))\n",
        "\n",
        "if net_d_fullfilename != '':\n",
        "    netD.load_state_dict(torch.load(net_d_fullfilename))\n",
        "\n",
        "logger.info(netD)\n",
        "logger.info(netG)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    os.makedirs(out_folder, exist_ok=True)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    real_label = 1\n",
        "    fake_label = 0\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=num_workers)\n",
        "\n",
        "    # set up optimizer\n",
        "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "\n",
        "    dataloader_size = len(dataloader)\n",
        "    \n",
        "    logger.info(\"Running iterations {} to {}\".format(min_iter, max_iter))\n",
        "    \n",
        "    for epoch in range(min_iter, max_iter):\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "            # train with real\n",
        "            netD.zero_grad()\n",
        "            real_cpu = data[0].to(device)\n",
        "            batch_size = real_cpu.size(0)\n",
        "            label = torch.full((batch_size,), real_label, device=device)\n",
        "\n",
        "            output = netD(real_cpu)\n",
        "            errD_real = criterion(output, label)\n",
        "            errD_real.backward()\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "            # train with fake\n",
        "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "            fake = netG(noise)\n",
        "            label.fill_(fake_label)\n",
        "            output = netD(fake.detach())\n",
        "            errD_fake = criterion(output, label)\n",
        "            errD_fake.backward()\n",
        "            D_G_z1 = output.mean().item()\n",
        "            errD = errD_real + errD_fake\n",
        "            optimizerD.step()\n",
        "\n",
        "            # (2) Update G network: maximize log(D(G(z)))\n",
        "            netG.zero_grad()\n",
        "            label.fill_(real_label)  # fake labels are real for generator cost\n",
        "            output = netD(fake)\n",
        "            errG = criterion(output, label)\n",
        "            errG.backward()\n",
        "            D_G_z2 = output.mean().item()\n",
        "            optimizerG.step()\n",
        "\n",
        "            if i % print_every_iteration == 0:\n",
        "                loss_d = errD.item()\n",
        "                loss_g = errG.item()\n",
        "\n",
        "                logger.info('[%d/%d] [%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "                      % (epoch, max_iter, i, dataloader_size, loss_d, loss_g, D_x, D_G_z1, D_G_z2))\n",
        "            if i % save_every_iteration == 0:\n",
        "                real_filename = '%s/real_samples.png' % out_folder\n",
        "                fake_filename = '%s/fake_samples_epoch_%03d.png' % (out_folder, epoch)\n",
        "\n",
        "                fake = netG(fixed_noise)\n",
        "                \n",
        "                vutils.save_image(real_cpu, real_filename, normalize=True)\n",
        "                vutils.save_image(fake.detach(), fake_filename, normalize=True)\n",
        "                \n",
        "        g_filename = checkpoint_pattern.format(g_pattern, epoch)\n",
        "        d_filename = checkpoint_pattern.format(d_pattern, epoch)\n",
        "\n",
        "        g_full_filename = os.path.join(checkpoint_dir, g_filename)\n",
        "        d_full_filename = os.path.join(checkpoint_dir, d_filename)\n",
        "\n",
        "        torch.save(netG.state_dict(), g_full_filename)\n",
        "        torch.save(netD.state_dict(), d_full_filename)\n",
        "\n",
        "        if 0 == epoch % checkpoint_every_epoch:\n",
        "            logger.info('Checkpointing epoch {}'.format(epoch))\n",
        "\n",
        "            # Delete any existing files with this name, to avoid ending up with multiple files\n",
        "            _delete_all_remote_files(g_filename)\n",
        "            _delete_all_remote_files(d_filename)\n",
        "            \n",
        "            gdrive_checkpoint.core.upload_file_to_folder(g_filename, folder=parent_fid)\n",
        "            gdrive_checkpoint.core.upload_file_to_folder(d_filename, folder=parent_fid) "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-15 10:38:04,852 496 140124913129344: Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n",
            "2019-04-15 10:38:04,854 496 140124913129344: Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n",
            "2019-04-15 10:38:04,857 496 140124913129344: Running iterations 1 to 3\n",
            "2019-04-15 10:38:05,020 496 140124913129344: [1/3] [0/3125] Loss_D: 1.7597 Loss_G: 7.2637 D(x): 0.9929 D(G(z)): 0.7379 / 0.0011\n",
            "2019-04-15 10:38:10,817 496 140124913129344: [1/3] [100/3125] Loss_D: 0.0944 Loss_G: 4.3760 D(x): 0.9777 D(G(z)): 0.0656 / 0.0196\n",
            "2019-04-15 10:38:16,458 496 140124913129344: [1/3] [200/3125] Loss_D: 2.8063 Loss_G: 2.7706 D(x): 0.1001 D(G(z)): 0.0024 / 0.1272\n",
            "2019-04-15 10:38:22,165 496 140124913129344: [1/3] [300/3125] Loss_D: 0.8054 Loss_G: 6.2239 D(x): 0.9806 D(G(z)): 0.5164 / 0.0027\n",
            "2019-04-15 10:38:27,839 496 140124913129344: [1/3] [400/3125] Loss_D: 0.2491 Loss_G: 6.1177 D(x): 0.9459 D(G(z)): 0.1489 / 0.0025\n",
            "2019-04-15 10:38:33,543 496 140124913129344: [1/3] [500/3125] Loss_D: 1.2268 Loss_G: 6.8398 D(x): 0.9984 D(G(z)): 0.5868 / 0.0020\n",
            "2019-04-15 10:38:39,192 496 140124913129344: [1/3] [600/3125] Loss_D: 1.3622 Loss_G: 1.2374 D(x): 0.5787 D(G(z)): 0.3419 / 0.3582\n",
            "2019-04-15 10:38:44,939 496 140124913129344: [1/3] [700/3125] Loss_D: 0.1124 Loss_G: 4.1136 D(x): 0.9046 D(G(z)): 0.0094 / 0.0207\n",
            "2019-04-15 10:38:50,605 496 140124913129344: [1/3] [800/3125] Loss_D: 0.2570 Loss_G: 3.0388 D(x): 0.8576 D(G(z)): 0.0844 / 0.0578\n",
            "2019-04-15 10:38:56,323 496 140124913129344: [1/3] [900/3125] Loss_D: 0.7179 Loss_G: 1.6709 D(x): 0.5715 D(G(z)): 0.0324 / 0.2542\n",
            "2019-04-15 10:39:01,983 496 140124913129344: [1/3] [1000/3125] Loss_D: 0.6695 Loss_G: 5.3300 D(x): 0.5740 D(G(z)): 0.0022 / 0.0103\n",
            "2019-04-15 10:39:07,729 496 140124913129344: [1/3] [1100/3125] Loss_D: 0.1875 Loss_G: 3.0400 D(x): 0.8929 D(G(z)): 0.0621 / 0.0643\n",
            "2019-04-15 10:39:13,420 496 140124913129344: [1/3] [1200/3125] Loss_D: 0.0343 Loss_G: 6.0413 D(x): 0.9726 D(G(z)): 0.0061 / 0.0057\n",
            "2019-04-15 10:39:19,162 496 140124913129344: [1/3] [1300/3125] Loss_D: 0.1016 Loss_G: 5.3097 D(x): 0.9913 D(G(z)): 0.0840 / 0.0060\n",
            "2019-04-15 10:39:24,844 496 140124913129344: [1/3] [1400/3125] Loss_D: 0.1729 Loss_G: 4.4508 D(x): 0.8787 D(G(z)): 0.0268 / 0.0198\n",
            "2019-04-15 10:39:30,562 496 140124913129344: [1/3] [1500/3125] Loss_D: 1.0825 Loss_G: 6.5430 D(x): 0.7774 D(G(z)): 0.4961 / 0.0019\n",
            "2019-04-15 10:39:36,246 496 140124913129344: [1/3] [1600/3125] Loss_D: 0.0956 Loss_G: 3.8925 D(x): 0.9242 D(G(z)): 0.0126 / 0.0306\n",
            "2019-04-15 10:39:42,002 496 140124913129344: [1/3] [1700/3125] Loss_D: 0.5564 Loss_G: 3.1750 D(x): 0.6502 D(G(z)): 0.0250 / 0.0767\n",
            "2019-04-15 10:39:47,738 496 140124913129344: [1/3] [1800/3125] Loss_D: 0.3749 Loss_G: 4.5906 D(x): 0.7167 D(G(z)): 0.0031 / 0.0220\n",
            "2019-04-15 10:39:53,447 496 140124913129344: [1/3] [1900/3125] Loss_D: 0.2067 Loss_G: 3.2938 D(x): 0.8593 D(G(z)): 0.0360 / 0.0513\n",
            "2019-04-15 10:39:59,144 496 140124913129344: [1/3] [2000/3125] Loss_D: 0.0206 Loss_G: 5.6089 D(x): 0.9909 D(G(z)): 0.0113 / 0.0064\n",
            "2019-04-15 10:40:04,872 496 140124913129344: [1/3] [2100/3125] Loss_D: 1.9316 Loss_G: 8.3698 D(x): 0.9995 D(G(z)): 0.8067 / 0.0003\n",
            "2019-04-15 10:40:10,558 496 140124913129344: [1/3] [2200/3125] Loss_D: 0.8114 Loss_G: 1.7254 D(x): 0.5136 D(G(z)): 0.0962 / 0.2460\n",
            "2019-04-15 10:40:16,307 496 140124913129344: [1/3] [2300/3125] Loss_D: 0.8453 Loss_G: 4.0533 D(x): 0.5100 D(G(z)): 0.0105 / 0.0369\n",
            "2019-04-15 10:40:21,985 496 140124913129344: [1/3] [2400/3125] Loss_D: 0.8564 Loss_G: 3.2212 D(x): 0.7556 D(G(z)): 0.3543 / 0.0453\n",
            "2019-04-15 10:40:27,711 496 140124913129344: [1/3] [2500/3125] Loss_D: 0.1582 Loss_G: 5.2947 D(x): 0.9790 D(G(z)): 0.1180 / 0.0071\n",
            "2019-04-15 10:40:33,428 496 140124913129344: [1/3] [2600/3125] Loss_D: 0.1478 Loss_G: 3.7339 D(x): 0.9792 D(G(z)): 0.1054 / 0.0418\n",
            "2019-04-15 10:40:39,136 496 140124913129344: [1/3] [2700/3125] Loss_D: 0.2071 Loss_G: 4.5866 D(x): 0.8392 D(G(z)): 0.0053 / 0.0163\n",
            "2019-04-15 10:40:44,826 496 140124913129344: [1/3] [2800/3125] Loss_D: 0.9890 Loss_G: 6.3490 D(x): 0.9989 D(G(z)): 0.4995 / 0.0031\n",
            "2019-04-15 10:40:50,541 496 140124913129344: [1/3] [2900/3125] Loss_D: 0.3466 Loss_G: 5.0673 D(x): 0.9599 D(G(z)): 0.2129 / 0.0105\n",
            "2019-04-15 10:40:56,199 496 140124913129344: [1/3] [3000/3125] Loss_D: 0.1123 Loss_G: 4.5249 D(x): 0.9624 D(G(z)): 0.0688 / 0.0172\n",
            "2019-04-15 10:41:01,934 496 140124913129344: [1/3] [3100/3125] Loss_D: 0.7136 Loss_G: 2.1339 D(x): 0.7795 D(G(z)): 0.2549 / 0.1561\n",
            "2019-04-15 10:41:03,379 496 140124913129344: Checkpointing epoch 1\n",
            "2019-04-15 10:41:03,384 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_z100_cifar10_epoch_0001.pth%22and+trashed+%3D+false&alt=json\n",
            "2019-04-15 10:41:03,936 496 140124913129344: Deleting GDriveItem(name='netG_z100_cifar10_epoch_0001.pth', fid='13qo4lIu5Ccq_2GO7n2nS4bznpTH40aUU')\n",
            "2019-04-15 10:41:03,941 496 140124913129344: URL being requested: DELETE https://www.googleapis.com/drive/v3/files/13qo4lIu5Ccq_2GO7n2nS4bznpTH40aUU?\n",
            "2019-04-15 10:41:04,523 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_z100_cifar10_epoch_0001.pth%22and+trashed+%3D+false&alt=json\n",
            "2019-04-15 10:41:04,863 496 140124913129344: Deleting GDriveItem(name='netD_z100_cifar10_epoch_0001.pth', fid='1_O3Rx_JVWKtzZJYuJHSpYYA2sTryhQtg')\n",
            "2019-04-15 10:41:04,869 496 140124913129344: URL being requested: DELETE https://www.googleapis.com/drive/v3/files/1_O3Rx_JVWKtzZJYuJHSpYYA2sTryhQtg?\n",
            "2019-04-15 10:41:05,445 496 140124913129344: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n",
            "2019-04-15 10:41:07,302 496 140124913129344: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n",
            "2019-04-15 10:41:09,134 496 140124913129344: [2/3] [0/3125] Loss_D: 0.9116 Loss_G: 2.5370 D(x): 0.5940 D(G(z)): 0.1829 / 0.1079\n",
            "2019-04-15 10:41:14,990 496 140124913129344: [2/3] [100/3125] Loss_D: 2.4488 Loss_G: 0.0535 D(x): 0.1424 D(G(z)): 0.0059 / 0.9493\n",
            "2019-04-15 10:41:20,648 496 140124913129344: [2/3] [200/3125] Loss_D: 0.2321 Loss_G: 3.4135 D(x): 0.9837 D(G(z)): 0.1849 / 0.0395\n",
            "2019-04-15 10:41:26,431 496 140124913129344: [2/3] [300/3125] Loss_D: 0.2828 Loss_G: 5.2247 D(x): 0.9982 D(G(z)): 0.2203 / 0.0082\n",
            "2019-04-15 10:41:32,117 496 140124913129344: [2/3] [400/3125] Loss_D: 0.1728 Loss_G: 3.4572 D(x): 0.8648 D(G(z)): 0.0148 / 0.0432\n",
            "2019-04-15 10:41:37,855 496 140124913129344: [2/3] [500/3125] Loss_D: 0.0813 Loss_G: 4.9606 D(x): 0.9870 D(G(z)): 0.0636 / 0.0087\n",
            "2019-04-15 10:41:43,522 496 140124913129344: [2/3] [600/3125] Loss_D: 0.3693 Loss_G: 2.6091 D(x): 0.8468 D(G(z)): 0.1591 / 0.1019\n",
            "2019-04-15 10:41:49,256 496 140124913129344: [2/3] [700/3125] Loss_D: 0.2829 Loss_G: 4.2586 D(x): 0.8976 D(G(z)): 0.1449 / 0.0178\n",
            "2019-04-15 10:41:54,942 496 140124913129344: [2/3] [800/3125] Loss_D: 0.0196 Loss_G: 5.3344 D(x): 0.9924 D(G(z)): 0.0118 / 0.0073\n",
            "2019-04-15 10:42:00,728 496 140124913129344: [2/3] [900/3125] Loss_D: 0.0825 Loss_G: 4.8826 D(x): 0.9627 D(G(z)): 0.0426 / 0.0103\n",
            "2019-04-15 10:42:06,473 496 140124913129344: [2/3] [1000/3125] Loss_D: 0.5315 Loss_G: 4.3048 D(x): 0.8518 D(G(z)): 0.2589 / 0.0178\n",
            "2019-04-15 10:42:12,273 496 140124913129344: [2/3] [1100/3125] Loss_D: 0.1855 Loss_G: 4.4336 D(x): 0.9890 D(G(z)): 0.1511 / 0.0167\n",
            "2019-04-15 10:42:18,019 496 140124913129344: [2/3] [1200/3125] Loss_D: 2.4992 Loss_G: 7.2835 D(x): 0.9982 D(G(z)): 0.8894 / 0.0009\n",
            "2019-04-15 10:42:23,822 496 140124913129344: [2/3] [1300/3125] Loss_D: 0.0787 Loss_G: 4.5543 D(x): 0.9879 D(G(z)): 0.0602 / 0.0156\n",
            "2019-04-15 10:42:29,558 496 140124913129344: [2/3] [1400/3125] Loss_D: 0.1186 Loss_G: 5.3785 D(x): 0.9989 D(G(z)): 0.1085 / 0.0053\n",
            "2019-04-15 10:42:35,337 496 140124913129344: [2/3] [1500/3125] Loss_D: 0.0192 Loss_G: 5.3302 D(x): 0.9915 D(G(z)): 0.0104 / 0.0066\n",
            "2019-04-15 10:42:41,084 496 140124913129344: [2/3] [1600/3125] Loss_D: 1.4487 Loss_G: 5.4525 D(x): 0.2973 D(G(z)): 0.0010 / 0.0068\n",
            "2019-04-15 10:42:46,825 496 140124913129344: [2/3] [1700/3125] Loss_D: 0.0682 Loss_G: 4.6813 D(x): 0.9979 D(G(z)): 0.0613 / 0.0154\n",
            "2019-04-15 10:42:52,540 496 140124913129344: [2/3] [1800/3125] Loss_D: 1.7913 Loss_G: 0.0029 D(x): 0.2254 D(G(z)): 0.0333 / 0.9972\n",
            "2019-04-15 10:42:58,349 496 140124913129344: [2/3] [1900/3125] Loss_D: 0.0316 Loss_G: 6.0938 D(x): 0.9790 D(G(z)): 0.0093 / 0.0035\n",
            "2019-04-15 10:43:04,099 496 140124913129344: [2/3] [2000/3125] Loss_D: 0.1485 Loss_G: 4.0727 D(x): 0.9475 D(G(z)): 0.0859 / 0.0230\n",
            "2019-04-15 10:43:09,851 496 140124913129344: [2/3] [2100/3125] Loss_D: 2.2729 Loss_G: 7.2154 D(x): 0.9879 D(G(z)): 0.8555 / 0.0012\n",
            "2019-04-15 10:43:15,599 496 140124913129344: [2/3] [2200/3125] Loss_D: 0.3665 Loss_G: 4.4531 D(x): 0.9882 D(G(z)): 0.2735 / 0.0129\n",
            "2019-04-15 10:43:21,379 496 140124913129344: [2/3] [2300/3125] Loss_D: 0.0160 Loss_G: 5.1177 D(x): 0.9970 D(G(z)): 0.0129 / 0.0078\n",
            "2019-04-15 10:43:27,101 496 140124913129344: [2/3] [2400/3125] Loss_D: 0.5224 Loss_G: 5.2555 D(x): 0.9704 D(G(z)): 0.3127 / 0.0077\n",
            "2019-04-15 10:43:32,896 496 140124913129344: [2/3] [2500/3125] Loss_D: 0.0255 Loss_G: 5.0886 D(x): 0.9972 D(G(z)): 0.0220 / 0.0108\n",
            "2019-04-15 10:43:38,611 496 140124913129344: [2/3] [2600/3125] Loss_D: 1.3568 Loss_G: 0.9435 D(x): 0.4163 D(G(z)): 0.0095 / 0.4386\n",
            "2019-04-15 10:43:44,408 496 140124913129344: [2/3] [2700/3125] Loss_D: 4.8689 Loss_G: 2.5359 D(x): 0.0121 D(G(z)): 0.0021 / 0.1073\n",
            "2019-04-15 10:43:50,120 496 140124913129344: [2/3] [2800/3125] Loss_D: 0.1378 Loss_G: 7.3866 D(x): 0.8816 D(G(z)): 0.0020 / 0.0016\n",
            "2019-04-15 10:43:55,909 496 140124913129344: [2/3] [2900/3125] Loss_D: 0.0121 Loss_G: 4.4489 D(x): 0.9967 D(G(z)): 0.0087 / 0.0177\n",
            "2019-04-15 10:44:01,647 496 140124913129344: [2/3] [3000/3125] Loss_D: 0.6034 Loss_G: 3.9206 D(x): 0.8221 D(G(z)): 0.2771 / 0.0455\n",
            "2019-04-15 10:44:07,418 496 140124913129344: [2/3] [3100/3125] Loss_D: 0.1778 Loss_G: 4.5115 D(x): 0.8957 D(G(z)): 0.0488 / 0.0162\n",
            "2019-04-15 10:44:08,874 496 140124913129344: Checkpointing epoch 2\n",
            "2019-04-15 10:44:08,886 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netG_z100_cifar10_epoch_0002.pth%22and+trashed+%3D+false&alt=json\n",
            "2019-04-15 10:44:09,244 496 140124913129344: Deleting GDriveItem(name='netG_z100_cifar10_epoch_0002.pth', fid='18RUgQWH5j4G6m5qPoSItXiv7NNWF7q4z')\n",
            "2019-04-15 10:44:09,249 496 140124913129344: URL being requested: DELETE https://www.googleapis.com/drive/v3/files/18RUgQWH5j4G6m5qPoSItXiv7NNWF7q4z?\n",
            "2019-04-15 10:44:09,814 496 140124913129344: URL being requested: GET https://www.googleapis.com/drive/v3/files?q=name+contains+%22netD_z100_cifar10_epoch_0002.pth%22and+trashed+%3D+false&alt=json\n",
            "2019-04-15 10:44:10,128 496 140124913129344: Deleting GDriveItem(name='netD_z100_cifar10_epoch_0002.pth', fid='1poWZEHiBRYaxFTKLBhCnaJCvRAONltQ7')\n",
            "2019-04-15 10:44:10,133 496 140124913129344: URL being requested: DELETE https://www.googleapis.com/drive/v3/files/1poWZEHiBRYaxFTKLBhCnaJCvRAONltQ7?\n",
            "2019-04-15 10:44:10,732 496 140124913129344: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n",
            "2019-04-15 10:44:12,610 496 140124913129344: URL being requested: POST https://www.googleapis.com/upload/drive/v3/files?fields=id&alt=json&uploadType=resumable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7berrnr383OD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "61b8bf31-6513-4b2b-ce34-88feffacdf5e"
      },
      "cell_type": "code",
      "source": [
        "logger.info(net_g_fullfilename)\n",
        "logger.info(net_d_fullfilename)\n",
        "!ls"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-15 11:20:17,197 496 140124913129344: netG_z100_cifar10_epoch_0001.pth\n",
            "2019-04-15 11:20:17,201 496 140124913129344: netD_z100_cifar10_epoch_0001.pth\n",
            "adc.json\t\t\t  netD_z100_cifar10_epoch_0001.pth\n",
            "examples\t\t\t  netD_z100_cifar10_epoch_0002.pth\n",
            "fake\t\t\t\t  netG_z100_cifar10_epoch_0000.pth\n",
            "fake_samples_epoch_000.png\t  netG_z100_cifar10_epoch_0001.pth\n",
            "fake_samples_epoch_001.png\t  netG_z100_cifar10_epoch_0002.pth\n",
            "fake_samples_epoch_002.png\t  real\n",
            "gan_metrics\t\t\t  real_samples.png\n",
            "gdrive_checkpoint\t\t  sample_data\n",
            "netD_z100_cifar10_epoch_0000.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NWpPT4drX5yQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "a5e92f18-5ae3-47ac-f226-b0cf8edc57ca"
      },
      "cell_type": "code",
      "source": [
        "# [emd-mmd-knn(knn,real,fake,precision,recall)]*4 - IS - mode_score - FID\n",
        "# score_tr = np.zeros((max_iter, 4*7+3))\n",
        "\n",
        "sample_size = 2000\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "# [emd-mmd-knn(knn,real,fake,precision,recall)]*4 - IS - mode_score - FID\n",
        "score_tr = np.zeros((max_iter, 4*7+3))\n",
        "\n",
        "for epoch in range(0, max_iter):\n",
        "    g_filename = checkpoint_pattern.format(g_pattern, epoch)\n",
        "\n",
        "    logger.info(g_filename)\n",
        "    netG.load_state_dict(torch.load(g_filename))\n",
        "    s = metric.compute_score_raw(dataset_name, \n",
        "                                 image_size, \n",
        "                                 dataroot, \n",
        "                                 sample_size, \n",
        "                                 batch_size, \n",
        "                                 saveFolder_r=saveFolder_r, \n",
        "                                 saveFolder_f=saveFolder_f, \n",
        "                                 netG=netG, \n",
        "                                 nz=nz, \n",
        "                                 conv_model='inception_v3', \n",
        "                                 workers=num_workers)\n",
        "    score_tr[epoch, :] = s\n",
        "\n",
        "      \n",
        "# save final metric scores of all epoches\n",
        "save_fn = '%s/score_tr_ep.npy' % out_folder\n",
        "np.save(save_fn, score_tr)\n",
        "logger.info('##### training completed#####')\n",
        "logger.info('### metric scores output is scored at {} ###'.format(save_fn))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-15 11:40:59,547 496 140124913129344: netG_z100_cifar10_epoch_0000.pth\n",
            "sampling real images ...\n",
            "Files already downloaded and verified\n",
            "sampling fake images ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/125 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting features...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/gan_metrics/metric.py:213: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  fsmax = F.softmax(flogit)\n",
            "100%|| 125/125 [00:19<00:00,  6.42it/s]\n",
            "  0%|          | 0/125 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting features...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 125/125 [00:19<00:00,  6.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "compute score in space: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yctPegCoY-2M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84635876-ffbf-469f-cf77-31f4824a7fbb"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "score_tr[:, 0]\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([207.63281182, 185.66896675, 199.18911655])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "sJ6rWgMge4-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !ls MNIST_DCGAN_results/Fixed_results/\n",
        "# !ls MNIST_DCGAN_results/Random_results/\n",
        "# !ls\n",
        "# os.path.split(g_filename)[1]\n",
        "!ls -ltra\n",
        "# net_g_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiiDugNNfjqg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# filename = 'fake_samples_epoch_001.png'\n",
        "filename = 'fake_samples_epoch_016.png'\n",
        "img = matplotlib.image.imread(filename)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PLrWLJsrAIwz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/xuqiantong/GAN-Metrics.git gan_metric\n",
        "# # !ls\n",
        "# import gan_metric.metric as metric\n",
        "# # import metric\n",
        "\n",
        "print(saveFolder_r)\n",
        "\n",
        "print(saveFolder_f)\n",
        "!ls real\n",
        "!ls fake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PKIyTqHoUTDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# https://github.com/kylematoba/GAN-Metrics/blob/master/metric.py#L399\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pfH39rFUKgJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wnJqiE4pTkvk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "saveFolder_r = os.path.join(out_folder, 'real')\n",
        "print(saveFolder_r)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}